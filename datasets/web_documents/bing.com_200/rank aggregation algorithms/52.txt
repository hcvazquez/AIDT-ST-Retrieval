<!doctype html>
<html class="client-nojs" lang="en" dir="ltr">
 <head> 
  <meta charset="UTF-8"> 
  <title>Ensemble learning - Wikipedia</title> 
  <script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script> 
  <script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Ensemble_learning","wgTitle":"Ensemble learning","wgCurRevisionId":793794298,"wgRevisionId":793794298,"wgArticleId":22212276,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["All articles with unsourced statements","Articles with unsourced statements from January 2012","Ensemble learning"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Ensemble_learning","wgRelevantArticleId":22212276,"wgRequestId":"Wdgk7ApAMFwAAENKE-YAAABG","wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgWikiEditorEnabledModules":{"toolbar":true,"preview":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":false,"wgPopupsShouldSendModuleToUser":false,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgPreferredVariant":"en","wgMFExpandAllSectionsUserOption":false,"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikibaseItemId":"Q245652","wgCentralAuthMobileDomain":false,"wgCodeMirrorEnabled":false,"wgVisualEditorToolbarScrollOffset":0,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":false});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready"});mw.loader.implement("user.tokens@1dqfd7l",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/

});mw.loader.load(["ext.cite.a11y","ext.math.scripts","site","mediawiki.page.startup","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.interface","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"]);});</script> 
  <link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.sectionAnchor%7Cmediawiki.skinning.interface%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"> 
  <script async src="/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script> 
  <meta name="ResourceLoaderDynamicStyles" content=""> 
  <link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector"> 
  <link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"> 
  <meta name="generator" content="MediaWiki 1.31.0-wmf.2"> 
  <meta name="referrer" content="origin-when-cross-origin"> 
  <meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1200px-Kernel_Machine.svg.png"> 
  <link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Ensemble_learning"> 
  <link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Ensemble_learning&amp;action=edit"> 
  <link rel="edit" title="Edit this page" href="/w/index.php?title=Ensemble_learning&amp;action=edit"> 
  <link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"> 
  <link rel="shortcut icon" href="/static/favicon/wikipedia.ico"> 
  <link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"> 
  <link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"> 
  <link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"> 
  <link rel="canonical" href="https://en.wikipedia.org/wiki/Ensemble_learning"> 
  <link rel="dns-prefetch" href="//login.wikimedia.org"> 
  <link rel="dns-prefetch" href="//meta.wikimedia.org"> 
  <!--[if lt IE 9]><script src="/resources/lib/html5shiv/html5shiv.min.js"></script><![endif]--> 
 </head> 
 <body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-Ensemble_learning rootpage-Ensemble_learning vector-nav-directionality skin-vector action-view"> 
  <div id="mw-page-base" class="noprint"></div> 
  <div id="mw-head-base" class="noprint"></div> 
  <div id="content" class="mw-body" role="main"> 
   <a id="top"></a> 
   <div id="siteNotice" class="mw-body-content">
    <!-- CentralNotice -->
   </div> 
   <div class="mw-indicators mw-body-content"> 
   </div> 
   <h1 id="firstHeading" class="firstHeading" lang="en">Ensemble learning</h1> 
   <div id="bodyContent" class="mw-body-content"> 
    <div id="siteSub" class="noprint">
     From Wikipedia, the free encyclopedia
    </div> 
    <div id="contentSub"></div> 
    <div id="jump-to-nav" class="mw-jump">
      Jump to: 
     <a href="#mw-head">navigation</a>, 
     <a href="#p-search">search</a> 
    </div> 
    <div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr">
     <div class="mw-parser-output">
      <div role="note" class="hatnote navigation-not-searchable">
       For an alternative meaning, see 
       <a href="/wiki/Variational_Bayesian_methods" title="Variational Bayesian methods">variational Bayesian methods</a>.
      </div> 
      <table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"> 
       <tbody>
        <tr> 
         <th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br> <a href="/wiki/Data_mining" title="Data mining">data mining</a></th> 
        </tr> 
        <tr> 
         <td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="/wiki/File:Kernel_Machine.svg" class="image"><img alt="Kernel Machine.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" width="220" height="100" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="233"></a></td> 
        </tr> 
        <tr> 
         <td style="padding:0 0.1em 0.4em"> 
          <div class="NavFrame collapsed" style="border:none;padding:0"> 
           <div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
            Problems
           </div> 
           <div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"> 
            <div class="hlist"> 
             <ul> 
              <li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li> 
              <li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li> 
              <li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li> 
              <li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li> 
              <li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li> 
              <li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li> 
              <li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li> 
              <li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li> 
              <li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li> 
              <li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li> 
              <li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li> 
              <li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li> 
              <li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li> 
              <li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li> 
             </ul> 
            </div> 
           </div> 
          </div> </td> 
        </tr> 
        <tr> 
         <td style="padding:0 0.1em 0.4em"> 
          <div class="NavFrame collapsed" style="border:none;padding:0"> 
           <div class="NavHead" style="font-size:105%;background:transparent;text-align:left"> 
            <div style="padding:0.1em 0;line-height:1.2em;">
             <a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a>
             <br> 
             <span style="font-weight:normal;"><small>(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&nbsp;• <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</small></span>
            </div> 
           </div> 
           <div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"> 
            <div class="hlist"> 
             <ul> 
              <li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li> 
              <li><a class="mw-selflink selflink">Ensembles</a> (<a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a>, <a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a>, <a href="/wiki/Random_forest" title="Random forest">Random forest</a>)</li> 
              <li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li> 
              <li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li> 
              <li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li> 
              <li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Neural networks</a></li> 
              <li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li> 
              <li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li> 
              <li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li> 
              <li><a href="/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li> 
             </ul> 
            </div> 
           </div> 
          </div> </td> 
        </tr> 
        <tr> 
         <td style="padding:0 0.1em 0.4em"> 
          <div class="NavFrame collapsed" style="border:none;padding:0"> 
           <div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
            <a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a>
           </div> 
           <div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"> 
            <div class="hlist"> 
             <ul> 
              <li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li> 
              <li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li> 
              <li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li> 
              <li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li> 
              <li><br> <a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li> 
              <li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li> 
              <li><a href="/wiki/Mean-shift" class="mw-redirect" title="Mean-shift">Mean-shift</a></li> 
             </ul> 
            </div> 
           </div> 
          </div> </td> 
        </tr> 
        <tr> 
         <td style="padding:0 0.1em 0.4em"> 
          <div class="NavFrame collapsed" style="border:none;padding:0"> 
           <div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
            <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a>
           </div> 
           <div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"> 
            <div class="hlist"> 
             <ul> 
              <li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li> 
              <li><a href="/wiki/Canonical_correlation_analysis" class="mw-redirect" title="Canonical correlation analysis">CCA</a></li> 
              <li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li> 
              <li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li> 
              <li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li> 
              <li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li> 
              <li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li> 
             </ul> 
            </div> 
           </div> 
          </div> </td> 
        </tr> 
        <tr> 
         <td style="padding:0 0.1em 0.4em"> 
          <div class="NavFrame collapsed" style="border:none;padding:0"> 
           <div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
            <a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a>
           </div> 
           <div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"> 
            <div class="hlist"> 
             <ul> 
              <li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a> (<a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a>, <a href="/wiki/Conditional_random_field" title="Conditional random field">CRF</a>, <a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">HMM</a>)</li> 
             </ul> 
            </div> 
           </div> 
          </div> </td> 
        </tr> 
        <tr> 
         <td style="padding:0 0.1em 0.4em"> 
          <div class="NavFrame collapsed" style="border:none;padding:0"> 
           <div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
            <a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a>
           </div> 
           <div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"> 
            <div class="hlist"> 
             <ul> 
              <li><a href="/wiki/K-nearest_neighbors_classification" class="mw-redirect" title="K-nearest neighbors classification"><i>k</i>-NN</a></li> 
              <li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li> 
             </ul> 
            </div> 
           </div> 
          </div> </td> 
        </tr> 
        <tr> 
         <td style="padding:0 0.1em 0.4em"> 
          <div class="NavFrame collapsed" style="border:none;padding:0"> 
           <div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
            <a href="/wiki/Artificial_neural_network" title="Artificial neural network">Neural nets</a>
           </div> 
           <div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"> 
            <div class="hlist"> 
             <ul> 
              <li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li> 
              <li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li> 
              <li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li> 
              <li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a></li> 
              <li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li> 
              <li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li> 
              <li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a></li> 
             </ul> 
            </div> 
           </div> 
          </div> </td> 
        </tr> 
        <tr> 
         <td style="padding:0 0.1em 0.4em"> 
          <div class="NavFrame collapsed" style="border:none;padding:0"> 
           <div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
            <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a>
           </div> 
           <div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"> 
            <div class="hlist"> 
             <ul> 
              <li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li> 
              <li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li> 
              <li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li> 
             </ul> 
            </div> 
           </div> 
          </div> </td> 
        </tr> 
        <tr> 
         <td style="padding:0 0.1em 0.4em"> 
          <div class="NavFrame collapsed" style="border:none;padding:0"> 
           <div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
            Theory
           </div> 
           <div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"> 
            <div class="hlist"> 
             <ul> 
              <li><a href="/wiki/Bias-variance_dilemma" class="mw-redirect" title="Bias-variance dilemma">Bias-variance dilemma</a></li> 
              <li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li> 
              <li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li> 
              <li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li> 
              <li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li> 
              <li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li> 
              <li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li> 
             </ul> 
            </div> 
           </div> 
          </div> </td> 
        </tr> 
        <tr> 
         <td style="padding:0 0.1em 0.4em"> 
          <div class="NavFrame collapsed" style="border:none;padding:0"> 
           <div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
            Machine-learning venues
           </div> 
           <div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"> 
            <div class="hlist"> 
             <ul> 
              <li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NIPS</a></li> 
              <li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li> 
              <li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li> 
              <li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li> 
              <li><a rel="nofollow" class="external text" href="http://arxiv.org/list/cs.LG/recent">ArXiv:cs.LG</a></li> 
             </ul> 
            </div> 
           </div> 
          </div> </td> 
        </tr> 
        <tr> 
         <td style="padding:0 0.1em 0.4em"> 
          <div class="NavFrame collapsed" style="border:none;padding:0"> 
           <div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
            Related articles
           </div> 
           <div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"> 
            <div class="hlist"> 
             <ul> 
              <li><a href="/wiki/List_of_datasets_for_machine-learning_research" class="mw-redirect" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li> 
              <li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li> 
             </ul> 
            </div> 
           </div> 
          </div> </td> 
        </tr> 
        <tr> 
         <td class="plainlist" style="padding:0.3em 0.4em 0.3em;font-weight:bold;border-top: 1px solid #aaa; border-bottom: 1px solid #aaa;border-top:1px solid #aaa;border-bottom:1px solid #aaa;"> 
          <ul> 
           <li><a href="/wiki/File:Portal-puzzle.svg" class="image"><img alt="Portal-puzzle.svg" src="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/16px-Portal-puzzle.svg.png" width="16" height="14" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x" data-file-width="32" data-file-height="28"></a> <a href="/wiki/Portal:Machine_learning" title="Portal:Machine learning">Machine learning portal</a></li> 
          </ul> </td> 
        </tr> 
        <tr> 
         <td style="text-align:right;font-size:115%;padding-top: 0.6em;"> 
          <div class="plainlinks hlist navbar mini"> 
           <ul> 
            <li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li> 
            <li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li> 
            <li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li> 
           </ul> 
          </div> </td> 
        </tr> 
       </tbody>
      </table> 
      <p>In <a href="/wiki/Statistics" title="Statistics">statistics</a> and <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b>ensemble methods</b> use multiple learning algorithms to obtain better <a href="/wiki/Predictive_inference" title="Predictive inference">predictive performance</a> than could be obtained from any of the constituent learning algorithms alone.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">[1]</a></sup><sup id="cite_ref-2" class="reference"><a href="#cite_note-2">[2]</a></sup><sup id="cite_ref-Rokach2010_3-0" class="reference"><a href="#cite_note-Rokach2010-3">[3]</a></sup> Unlike a <a href="/wiki/Statistical_ensemble" class="mw-redirect" title="Statistical ensemble">statistical ensemble</a> in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.</p> 
      <p></p> 
      <div id="toc" class="toc"> 
       <div class="toctitle"> 
        <h2>Contents</h2> 
       </div> 
       <ul> 
        <li class="toclevel-1 tocsection-1"><a href="#Overview"><span class="tocnumber">1</span> <span class="toctext">Overview</span></a></li> 
        <li class="toclevel-1 tocsection-2"><a href="#Ensemble_theory"><span class="tocnumber">2</span> <span class="toctext">Ensemble theory</span></a></li> 
        <li class="toclevel-1 tocsection-3"><a href="#Ensemble_Size"><span class="tocnumber">3</span> <span class="toctext">Ensemble Size</span></a></li> 
        <li class="toclevel-1 tocsection-4"><a href="#Common_types_of_ensembles"><span class="tocnumber">4</span> <span class="toctext">Common types of ensembles</span></a> 
         <ul> 
          <li class="toclevel-2 tocsection-5"><a href="#Bayes_optimal_classifier"><span class="tocnumber">4.1</span> <span class="toctext">Bayes optimal classifier</span></a></li> 
          <li class="toclevel-2 tocsection-6"><a href="#Bootstrap_aggregating_.28bagging.29"><span class="tocnumber">4.2</span> <span class="toctext">Bootstrap aggregating (bagging)</span></a></li> 
          <li class="toclevel-2 tocsection-7"><a href="#Boosting"><span class="tocnumber">4.3</span> <span class="toctext">Boosting</span></a></li> 
          <li class="toclevel-2 tocsection-8"><a href="#Bayesian_parameter_averaging"><span class="tocnumber">4.4</span> <span class="toctext">Bayesian parameter averaging</span></a></li> 
          <li class="toclevel-2 tocsection-9"><a href="#Bayesian_model_combination"><span class="tocnumber">4.5</span> <span class="toctext">Bayesian model combination</span></a></li> 
          <li class="toclevel-2 tocsection-10"><a href="#Bucket_of_models"><span class="tocnumber">4.6</span> <span class="toctext">Bucket of models</span></a></li> 
          <li class="toclevel-2 tocsection-11"><a href="#Stacking"><span class="tocnumber">4.7</span> <span class="toctext">Stacking</span></a></li> 
         </ul> </li> 
        <li class="toclevel-1 tocsection-12"><a href="#Implementations_in_statistics_packages"><span class="tocnumber">5</span> <span class="toctext">Implementations in statistics packages</span></a></li> 
        <li class="toclevel-1 tocsection-13"><a href="#See_also"><span class="tocnumber">6</span> <span class="toctext">See also</span></a></li> 
        <li class="toclevel-1 tocsection-14"><a href="#References"><span class="tocnumber">7</span> <span class="toctext">References</span></a></li> 
        <li class="toclevel-1 tocsection-15"><a href="#Further_reading"><span class="tocnumber">8</span> <span class="toctext">Further reading</span></a></li> 
        <li class="toclevel-1 tocsection-16"><a href="#External_links"><span class="tocnumber">9</span> <span class="toctext">External links</span></a></li> 
       </ul> 
      </div> 
      <p></p> 
      <h2><span class="mw-headline" id="Overview">Overview</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=1" title="Edit section: Overview">edit</a><span class="mw-editsection-bracket">]</span></span></h2> 
      <p><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a> algorithms are most commonly described as performing the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem. Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis. The term <i>ensemble</i> is usually reserved for methods that generate multiple hypotheses using the same base learner. The broader term of <i>multiple classifier systems</i> also covers hybridization of hypotheses that are not induced by the same base learner.</p> 
      <p>Evaluating the prediction of an ensemble typically requires more computation than evaluating the prediction of a single model, so ensembles may be thought of as a way to compensate for poor learning algorithms by performing a lot of extra computation. Fast algorithms such as <a href="/wiki/Decision_tree_learning" title="Decision tree learning">decision trees</a> are commonly used in ensemble methods (for example <i><a href="/wiki/Random_forest" title="Random forest">Random Forest</a></i>), although slower algorithms can benefit from ensemble techniques as well.</p> 
      <p>By analogy, ensemble techniques have been used also in <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a> scenarios, for example in <a href="/wiki/Consensus_clustering" title="Consensus clustering">consensus clustering</a> or in <a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a>.</p> 
      <h2><span class="mw-headline" id="Ensemble_theory">Ensemble theory</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=2" title="Edit section: Ensemble theory">edit</a><span class="mw-editsection-bracket">]</span></span></h2> 
      <p>An ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to <a href="/wiki/Overfitting" title="Overfitting">over-fit</a> the training data more than a single model would, but in practice, some ensemble techniques (especially <a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">bagging</a>) tend to reduce problems related to over-fitting of the training data.</p> 
      <p>Empirically, ensembles tend to yield better results when there is a significant diversity among the models.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">[4]</a></sup><sup id="cite_ref-5" class="reference"><a href="#cite_note-5">[5]</a></sup> Many ensemble methods, therefore, seek to promote diversity among the models they combine.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">[6]</a></sup><sup id="cite_ref-7" class="reference"><a href="#cite_note-7">[7]</a></sup> Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees).<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">[8]</a></sup> Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to <i>dumb-down</i> the models in order to promote diversity.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">[9]</a></sup></p> 
      <h2><span class="mw-headline" id="Ensemble_Size">Ensemble Size</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=3" title="Edit section: Ensemble Size">edit</a><span class="mw-editsection-bracket">]</span></span></h2> 
      <p>While the number of component classifiers of an ensemble has a great impact on the accuracy of prediction, there is a limited number of studies addressing this problem. <i>A priori</i> determining of ensemble size and the volume and velocity of big data streams make this even more crucial for online ensemble classifiers. Mostly statistical tests were used for determining the proper number of components. More recently, a theoretical framework suggested that there is an ideal number of component classifiers for an ensemble which having more or less than this number of classifiers would deteriorate the accuracy. It is called "the law of diminishing returns in ensemble construction." Their theoretical framework shows that using the same number of independent component classifiers as class labels gives the highest accuracy.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">[10]</a></sup></p> 
      <h2><span class="mw-headline" id="Common_types_of_ensembles">Common types of ensembles</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=4" title="Edit section: Common types of ensembles">edit</a><span class="mw-editsection-bracket">]</span></span></h2> 
      <h3><span class="mw-headline" id="Bayes_optimal_classifier">Bayes optimal classifier</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=5" title="Edit section: Bayes optimal classifier">edit</a><span class="mw-editsection-bracket">]</span></span></h3> 
      <p>The Bayes Optimal Classifier is a classification technique. It is an ensemble of all the hypotheses in the hypothesis space. On average, no other ensemble can outperform it.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">[11]</a></sup> Each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a system if that hypothesis were true. To facilitate training data of finite size, the vote of each hypothesis is also multiplied by the prior probability of that hypothesis. The Bayes Optimal Classifier can be expressed with the following equation:</p> 
      <dl> 
       <dd>
        <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
          <math xmlns="http://www.w3.org/1998/Math/MathML"> 
           <semantics> 
            <mrow class="MJX-TeXAtom-ORD"> 
             <mstyle displaystyle="true" scriptlevel="0"> 
              <mi>
               y
              </mi> 
              <mo>
               =
              </mo> 
              <mrow class="MJX-TeXAtom-ORD"> 
               <munder> 
                <mrow class="MJX-TeXAtom-ORD"> 
                 <mi mathvariant="normal">
                  a
                 </mi> 
                 <mi mathvariant="normal">
                  r
                 </mi> 
                 <mi mathvariant="normal">
                  g
                 </mi> 
                 <mi mathvariant="normal">
                  m
                 </mi> 
                 <mi mathvariant="normal">
                  a
                 </mi> 
                 <mi mathvariant="normal">
                  x
                 </mi> 
                </mrow> 
                <mrow> 
                 <msub> 
                  <mi>
                   c
                  </mi> 
                  <mrow class="MJX-TeXAtom-ORD"> 
                   <mi>
                    j
                   </mi> 
                  </mrow> 
                 </msub> 
                 <mo>
                  ?
                  <!-- ? -->
                 </mo> 
                 <mi>
                  C
                 </mi> 
                </mrow> 
               </munder> 
              </mrow> 
              <munder> 
               <mo>
                ?
                <!-- ? -->
               </mo> 
               <mrow class="MJX-TeXAtom-ORD"> 
                <msub> 
                 <mi>
                  h
                 </mi> 
                 <mrow class="MJX-TeXAtom-ORD"> 
                  <mi>
                   i
                  </mi> 
                 </mrow> 
                </msub> 
                <mo>
                 ?
                 <!-- ? -->
                </mo> 
                <mi>
                 H
                </mi> 
               </mrow> 
              </munder> 
              <mrow class="MJX-TeXAtom-ORD"> 
               <mi>
                P
               </mi> 
               <mo stretchy="false">
                (
               </mo> 
               <msub> 
                <mi>
                 c
                </mi> 
                <mrow class="MJX-TeXAtom-ORD"> 
                 <mi>
                  j
                 </mi> 
                </mrow> 
               </msub> 
               <mrow class="MJX-TeXAtom-ORD"> 
                <mo stretchy="false">
                 |
                </mo> 
               </mrow> 
               <msub> 
                <mi>
                 h
                </mi> 
                <mrow class="MJX-TeXAtom-ORD"> 
                 <mi>
                  i
                 </mi> 
                </mrow> 
               </msub> 
               <mo stretchy="false">
                )
               </mo> 
               <mi>
                P
               </mi> 
               <mo stretchy="false">
                (
               </mo> 
               <mi>
                T
               </mi> 
               <mrow class="MJX-TeXAtom-ORD"> 
                <mo stretchy="false">
                 |
                </mo> 
               </mrow> 
               <msub> 
                <mi>
                 h
                </mi> 
                <mrow class="MJX-TeXAtom-ORD"> 
                 <mi>
                  i
                 </mi> 
                </mrow> 
               </msub> 
               <mo stretchy="false">
                )
               </mo> 
               <mi>
                P
               </mi> 
               <mo stretchy="false">
                (
               </mo> 
               <msub> 
                <mi>
                 h
                </mi> 
                <mrow class="MJX-TeXAtom-ORD"> 
                 <mi>
                  i
                 </mi> 
                </mrow> 
               </msub> 
               <mo stretchy="false">
                )
               </mo> 
              </mrow> 
             </mstyle> 
            </mrow> 
            <annotation encoding="application/x-tex">
             {\displaystyle y={\underset {c_{j}\in C}{\mathrm {argmax} }}\sum _{h_{i}\in H}{P(c_{j}|h_{i})P(T|h_{i})P(h_{i})}}
            </annotation> 
           </semantics> 
          </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/09892e2a0091cfa48b8662fbf4c5f196689693b8" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.505ex; width:38.953ex; height:6.009ex;" alt="{\displaystyle y={\underset {c_{j}\in C}{\mathrm {argmax} }}\sum _{h_{i}\in H}{P(c_{j}|h_{i})P(T|h_{i})P(h_{i})}}"></span>
       </dd> 
      </dl> 
      <p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
         <math xmlns="http://www.w3.org/1998/Math/MathML"> 
          <semantics> 
           <mrow class="MJX-TeXAtom-ORD"> 
            <mstyle displaystyle="true" scriptlevel="0"> 
             <mi>
              y
             </mi> 
            </mstyle> 
           </mrow> 
           <annotation encoding="application/x-tex">
            {\displaystyle y}
           </annotation> 
          </semantics> 
         </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.166ex; height:2.009ex;" alt="y"></span> is the predicted class, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
         <math xmlns="http://www.w3.org/1998/Math/MathML"> 
          <semantics> 
           <mrow class="MJX-TeXAtom-ORD"> 
            <mstyle displaystyle="true" scriptlevel="0"> 
             <mi>
              C
             </mi> 
            </mstyle> 
           </mrow> 
           <annotation encoding="application/x-tex">
            {\displaystyle C}
           </annotation> 
          </semantics> 
         </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4fc55753007cd3c18576f7933f6f089196732029" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.777ex; height:2.176ex;" alt="C"></span> is the set of all possible classes, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
         <math xmlns="http://www.w3.org/1998/Math/MathML"> 
          <semantics> 
           <mrow class="MJX-TeXAtom-ORD"> 
            <mstyle displaystyle="true" scriptlevel="0"> 
             <mi>
              H
             </mi> 
            </mstyle> 
           </mrow> 
           <annotation encoding="application/x-tex">
            {\displaystyle H}
           </annotation> 
          </semantics> 
         </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.074ex; height:2.176ex;" alt="H"></span> is the hypothesis space, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
         <math xmlns="http://www.w3.org/1998/Math/MathML"> 
          <semantics> 
           <mrow class="MJX-TeXAtom-ORD"> 
            <mstyle displaystyle="true" scriptlevel="0"> 
             <mi>
              P
             </mi> 
            </mstyle> 
           </mrow> 
           <annotation encoding="application/x-tex">
            {\displaystyle P}
           </annotation> 
          </semantics> 
         </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b4dc73bf40314945ff376bd363916a738548d40a" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.756ex; height:2.176ex;" alt="P"></span> refers to a <i>probability</i>, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
         <math xmlns="http://www.w3.org/1998/Math/MathML"> 
          <semantics> 
           <mrow class="MJX-TeXAtom-ORD"> 
            <mstyle displaystyle="true" scriptlevel="0"> 
             <mi>
              T
             </mi> 
            </mstyle> 
           </mrow> 
           <annotation encoding="application/x-tex">
            {\displaystyle T}
           </annotation> 
          </semantics> 
         </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ec7200acd984a1d3a3d7dc455e262fbe54f7f6e0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.647ex; height:2.176ex;" alt="T"></span> is the training data. As an ensemble, the Bayes Optimal Classifier represents a hypothesis that is not necessarily in <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
         <math xmlns="http://www.w3.org/1998/Math/MathML"> 
          <semantics> 
           <mrow class="MJX-TeXAtom-ORD"> 
            <mstyle displaystyle="true" scriptlevel="0"> 
             <mi>
              H
             </mi> 
            </mstyle> 
           </mrow> 
           <annotation encoding="application/x-tex">
            {\displaystyle H}
           </annotation> 
          </semantics> 
         </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.074ex; height:2.176ex;" alt="H"></span>. The hypothesis represented by the Bayes Optimal Classifier, however, is the optimal hypothesis in <i>ensemble space</i> (the space of all possible ensembles consisting only of hypotheses in <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
         <math xmlns="http://www.w3.org/1998/Math/MathML"> 
          <semantics> 
           <mrow class="MJX-TeXAtom-ORD"> 
            <mstyle displaystyle="true" scriptlevel="0"> 
             <mi>
              H
             </mi> 
            </mstyle> 
           </mrow> 
           <annotation encoding="application/x-tex">
            {\displaystyle H}
           </annotation> 
          </semantics> 
         </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.074ex; height:2.176ex;" alt="H"></span>).</p> 
      <p>Unfortunately, the Bayes Optimal Classifier cannot be practically implemented for any but the most simple of problems. There are several reasons why the Bayes Optimal Classifier cannot be practically implemented:</p> 
      <ol> 
       <li>Most interesting hypothesis spaces are too large to iterate over, as required by the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
          <math xmlns="http://www.w3.org/1998/Math/MathML"> 
           <semantics> 
            <mrow class="MJX-TeXAtom-ORD"> 
             <mstyle displaystyle="true" scriptlevel="0"> 
              <mrow class="MJX-TeXAtom-ORD"> 
               <mi mathvariant="normal">
                a
               </mi> 
               <mi mathvariant="normal">
                r
               </mi> 
               <mi mathvariant="normal">
                g
               </mi> 
               <mi mathvariant="normal">
                m
               </mi> 
               <mi mathvariant="normal">
                a
               </mi> 
               <mi mathvariant="normal">
                x
               </mi> 
              </mrow> 
             </mstyle> 
            </mrow> 
            <annotation encoding="application/x-tex">
             {\displaystyle \mathrm {argmax} }
            </annotation> 
           </semantics> 
          </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a0e0b170d68be429b556aabd0fe79d57c7249162" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:7.625ex; height:2.176ex;" alt="\mathrm{argmax}"></span>.</li> 
       <li>Many hypotheses yield only a predicted class, rather than a probability for each class as required by the term <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
          <math xmlns="http://www.w3.org/1998/Math/MathML"> 
           <semantics> 
            <mrow class="MJX-TeXAtom-ORD"> 
             <mstyle displaystyle="true" scriptlevel="0"> 
              <mi>
               P
              </mi> 
              <mo stretchy="false">
               (
              </mo> 
              <msub> 
               <mi>
                c
               </mi> 
               <mrow class="MJX-TeXAtom-ORD"> 
                <mi>
                 j
                </mi> 
               </mrow> 
              </msub> 
              <mrow class="MJX-TeXAtom-ORD"> 
               <mo stretchy="false">
                |
               </mo> 
              </mrow> 
              <msub> 
               <mi>
                h
               </mi> 
               <mrow class="MJX-TeXAtom-ORD"> 
                <mi>
                 i
                </mi> 
               </mrow> 
              </msub> 
              <mo stretchy="false">
               )
              </mo> 
             </mstyle> 
            </mrow> 
            <annotation encoding="application/x-tex">
             {\displaystyle P(c_{j}|h_{i})}
            </annotation> 
           </semantics> 
          </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/29a45033b5718a371bb7b2cec545776d1920a4a6" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:8.334ex; height:3.009ex;" alt="P(c_j|h_i)"></span>.</li> 
       <li>Computing an unbiased estimate of the probability of the training set given a hypothesis (<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
          <math xmlns="http://www.w3.org/1998/Math/MathML"> 
           <semantics> 
            <mrow class="MJX-TeXAtom-ORD"> 
             <mstyle displaystyle="true" scriptlevel="0"> 
              <mi>
               P
              </mi> 
              <mo stretchy="false">
               (
              </mo> 
              <mi>
               T
              </mi> 
              <mrow class="MJX-TeXAtom-ORD"> 
               <mo stretchy="false">
                |
               </mo> 
              </mrow> 
              <msub> 
               <mi>
                h
               </mi> 
               <mrow class="MJX-TeXAtom-ORD"> 
                <mi>
                 i
                </mi> 
               </mrow> 
              </msub> 
              <mo stretchy="false">
               )
              </mo> 
             </mstyle> 
            </mrow> 
            <annotation encoding="application/x-tex">
             {\displaystyle P(T|h_{i})}
            </annotation> 
           </semantics> 
          </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7e97cf0b35fcef3b859b51e53def6f40cf96e601" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.047ex; height:2.843ex;" alt="P(T|h_i)"></span>) is non-trivial.</li> 
       <li>Estimating the prior probability for each hypothesis (<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
          <math xmlns="http://www.w3.org/1998/Math/MathML"> 
           <semantics> 
            <mrow class="MJX-TeXAtom-ORD"> 
             <mstyle displaystyle="true" scriptlevel="0"> 
              <mi>
               P
              </mi> 
              <mo stretchy="false">
               (
              </mo> 
              <msub> 
               <mi>
                h
               </mi> 
               <mrow class="MJX-TeXAtom-ORD"> 
                <mi>
                 i
                </mi> 
               </mrow> 
              </msub> 
              <mo stretchy="false">
               )
              </mo> 
             </mstyle> 
            </mrow> 
            <annotation encoding="application/x-tex">
             {\displaystyle P(h_{i})}
            </annotation> 
           </semantics> 
          </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bb77d8d56bd460a965b1ab90a0fc6b49d925d07d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.743ex; height:2.843ex;" alt="P(h_i)"></span>) is rarely feasible.</li> 
      </ol> 
      <h3><span id="Bootstrap_aggregating_(bagging)"></span><span class="mw-headline" id="Bootstrap_aggregating_.28bagging.29">Bootstrap aggregating (bagging)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=6" title="Edit section: Bootstrap aggregating (bagging)">edit</a><span class="mw-editsection-bracket">]</span></span></h3> 
      <div role="note" class="hatnote navigation-not-searchable">
       Main article: 
       <a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bootstrap aggregating</a>
      </div> 
      <p>Bootstrap aggregating, often abbreviated as <i>bagging</i>, involves having each model in the ensemble vote with equal weight. In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. As an example, the <a href="/wiki/Random_forest" title="Random forest">random forest</a> algorithm combines random decision trees with bagging to achieve very high classification accuracy.<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">[12]</a></sup></p> 
      <h3><span class="mw-headline" id="Boosting">Boosting</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=7" title="Edit section: Boosting">edit</a><span class="mw-editsection-bracket">]</span></span></h3> 
      <div role="note" class="hatnote navigation-not-searchable">
       Main article: 
       <a href="/wiki/Boosting_(meta-algorithm)" class="mw-redirect" title="Boosting (meta-algorithm)">Boosting (meta-algorithm)</a>
      </div> 
      <p>Boosting involves incrementally building an ensemble by training each new model instance to emphasize the training instances that previous models mis-classified. In some cases, boosting has been shown to yield better accuracy than bagging, but it also tends to be more likely to over-fit the training data. By far, the most common implementation of Boosting is <a href="/wiki/Adaboost" class="mw-redirect" title="Adaboost">Adaboost</a>, although some newer algorithms are reported to achieve better results<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (January 2012)">citation needed</span></a></i>]</sup>.</p> 
      <h3><span class="mw-headline" id="Bayesian_parameter_averaging">Bayesian parameter averaging</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=8" title="Edit section: Bayesian parameter averaging">edit</a><span class="mw-editsection-bracket">]</span></span></h3> 
      <p>Bayesian parameter averaging (BPA) is an ensemble technique that seeks to approximate the Bayes Optimal Classifier by sampling hypotheses from the hypothesis space, and combining them using Bayes' law.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">[13]</a></sup> Unlike the Bayes optimal classifier, Bayesian model averaging (BMA) can be practically implemented. Hypotheses are typically sampled using a <a href="/wiki/Monte_Carlo_sampling" class="mw-redirect" title="Monte Carlo sampling">Monte Carlo sampling</a> technique such as <a href="/wiki/Markov_chain_Monte_Carlo" title="Markov chain Monte Carlo">MCMC</a>. For example, <a href="/wiki/Gibbs_sampling" title="Gibbs sampling">Gibbs sampling</a> may be used to draw hypotheses that are representative of the distribution <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;">
         <math xmlns="http://www.w3.org/1998/Math/MathML"> 
          <semantics> 
           <mrow class="MJX-TeXAtom-ORD"> 
            <mstyle displaystyle="true" scriptlevel="0"> 
             <mi>
              P
             </mi> 
             <mo stretchy="false">
              (
             </mo> 
             <mi>
              T
             </mi> 
             <mrow class="MJX-TeXAtom-ORD"> 
              <mo stretchy="false">
               |
              </mo> 
             </mrow> 
             <mi>
              H
             </mi> 
             <mo stretchy="false">
              )
             </mo> 
            </mstyle> 
           </mrow> 
           <annotation encoding="application/x-tex">
            {\displaystyle P(T|H)}
           </annotation> 
          </semantics> 
         </math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8860a1802bcfb0ff76035b51323073995d6733e9" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:7.964ex; height:2.843ex;" alt="P(T|H)"></span>. It has been shown that under certain circumstances, when hypotheses are drawn in this manner and averaged according to Bayes' law, this technique has an expected error that is bounded to be at most twice the expected error of the Bayes optimal classifier.<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">[14]</a></sup> Despite the theoretical correctness of this technique, early work showed experimental results suggesting that the method promoted over-fitting and performed worse compared to simpler ensemble techniques such as bagging;<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">[15]</a></sup> however, these conclusions appear to be based on a misunderstanding of the purpose of Bayesian model averaging vs. model combination.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">[16]</a></sup> Additionally, there have been considerable advances in theory and practice of BMA. Recent rigorous proofs demonstrate the accuracy of BMA in variable selection and estimation in high-dimensional settings,<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">[17]</a></sup> and provide empirical evidence highlighting the role of sparsity-enforcing priors within the BMA in alleviating overfitting.<sup id="cite_ref-18" class="reference"><a href="#cite_note-18">[18]</a></sup></p> 
      <h3><span class="mw-headline" id="Bayesian_model_combination">Bayesian model combination</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=9" title="Edit section: Bayesian model combination">edit</a><span class="mw-editsection-bracket">]</span></span></h3> 
      <p>Bayesian model combination (BMC) is an algorithmic correction to Bayesian model averaging (BMA). Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weightings drawn randomly from a Dirichlet distribution having uniform parameters). This modification overcomes the tendency of BMA to converge toward giving all of the weight to a single model. Although BMC is somewhat more computationally expensive than BMA, it tends to yield dramatically better results. The results from BMC have been shown to be better on average (with statistical significance) than BMA, and bagging.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">[19]</a></sup></p> 
      <p>The use of Bayes' law to compute model weights necessitates computing the probability of the data given each model. Typically, none of the models in the ensemble are exactly the distribution from which the training data were generated, so all of them correctly receive a value close to zero for this term. This would work well if the ensemble were big enough to sample the entire model-space, but such is rarely possible. Consequently, each pattern in the training data will cause the ensemble weight to shift toward the model in the ensemble that is closest to the distribution of the training data. It essentially reduces to an unnecessarily complex method for doing model selection.</p> 
      <p>The possible weightings for an ensemble can be visualized as lying on a simplex. At each vertex of the simplex, all of the weight is given to a single model in the ensemble. BMA converges toward the vertex that is closest to the distribution of the training data. By contrast, BMC converges toward the point where this distribution projects onto the simplex. In other words, instead of selecting the one model that is closest to the generating distribution, it seeks the combination of models that is closest to the generating distribution.</p> 
      <p>The results from BMA can often be approximated by using cross-validation to select the best model from a bucket of models. Likewise, the results from BMC may be approximated by using cross-validation to select the best ensemble combination from a random sampling of possible weightings.</p> 
      <h3><span class="mw-headline" id="Bucket_of_models">Bucket of models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=10" title="Edit section: Bucket of models">edit</a><span class="mw-editsection-bracket">]</span></span></h3> 
      <p>A "bucket of models" is an ensemble technique in which a model selection algorithm is used to choose the best model for each problem. When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set.</p> 
      <p>The most common approach used for model-selection is <a href="/wiki/Cross-validation_(statistics)" title="Cross-validation (statistics)">cross-validation</a> selection (sometimes called a "bake-off contest"). It is described with the following pseudo-code:</p> 
      <pre>
For each model m in the bucket:
  Do c times: (where 'c' is some constant)
    Randomly divide the training dataset into two datasets: A, and B.
    Train m with A
    Test m with B
Select the model that obtains the highest average score
</pre> 
      <p>Cross-Validation Selection can be summed up as: "try them all with the training set, and pick the one that works best".<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">[20]</a></sup></p> 
      <p>Gating is a generalization of Cross-Validation Selection. It involves training another learning model to decide which of the models in the bucket is best-suited to solve the problem. Often, a <a href="/wiki/Perceptron" title="Perceptron">perceptron</a> is used for the gating model. It can be used to pick the "best" model, or it can be used to give a linear weight to the predictions from each model in the bucket.</p> 
      <p>When a bucket of models is used with a large set of problems, it may be desirable to avoid training some of the models that take a long time to train. Landmark learning is a meta-learning approach that seeks to solve this problem. It involves training only the fast (but imprecise) algorithms in the bucket, and then using the performance of these algorithms to help determine which slow (but accurate) algorithm is most likely to do best.<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">[21]</a></sup></p> 
      <h3><span class="mw-headline" id="Stacking">Stacking</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=11" title="Edit section: Stacking">edit</a><span class="mw-editsection-bracket">]</span></span></h3> 
      <p>Stacking (sometimes called <i>stacked generalization</i>) involves training a learning algorithm to combine the predictions of several other learning algorithms. First, all of the other algorithms are trained using the available data, then a combiner algorithm is trained to make a final prediction using all the predictions of the other algorithms as additional inputs. If an arbitrary combiner algorithm is used, then stacking can theoretically represent any of the ensemble techniques described in this article, although in practice, a single-layer <a href="/wiki/Logistic_regression" title="Logistic regression">logistic regression</a> model is often used as the combiner.</p> 
      <p>Stacking typically yields performance better than any single one of the trained models.<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">[22]</a></sup> It has been successfully used on both supervised learning tasks (regression,<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">[23]</a></sup> classification and distance learning <sup id="cite_ref-24" class="reference"><a href="#cite_note-24">[24]</a></sup>) and unsupervised learning (density estimation).<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">[25]</a></sup> It has also been used to estimate bagging's error rate.<sup id="cite_ref-Rokach2010_3-1" class="reference"><a href="#cite_note-Rokach2010-3">[3]</a></sup><sup id="cite_ref-26" class="reference"><a href="#cite_note-26">[26]</a></sup> It has been reported to out-perform Bayesian model-averaging.<sup id="cite_ref-27" class="reference"><a href="#cite_note-27">[27]</a></sup> The two top-performers in the Netflix competition utilized <i>blending</i>, which may be considered to be a form of stacking.<sup id="cite_ref-28" class="reference"><a href="#cite_note-28">[28]</a></sup></p> 
      <h2><span class="mw-headline" id="Implementations_in_statistics_packages">Implementations in statistics packages</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=12" title="Edit section: Implementations in statistics packages">edit</a><span class="mw-editsection-bracket">]</span></span></h2> 
      <ul> 
       <li><a href="/wiki/R_(programming_language)" title="R (programming language)">R</a>: at least three packages offer Bayesian model averaging tools,<sup id="cite_ref-29" class="reference"><a href="#cite_note-29">[29]</a></sup> including the <tt>BMS</tt> (an acronym for Bayesian Model Selection) package,<sup id="cite_ref-30" class="reference"><a href="#cite_note-30">[30]</a></sup> the <tt>BAS</tt> (an acronym for Bayesian Adaptive Sampling) package,<sup id="cite_ref-31" class="reference"><a href="#cite_note-31">[31]</a></sup> and the <tt>BMA</tt> package.<sup id="cite_ref-32" class="reference"><a href="#cite_note-32">[32]</a></sup></li> 
       <li><a href="/wiki/Python_(programming_language)" title="Python (programming language)">Python</a>&nbsp;: <a href="/wiki/Scikit-learn" title="Scikit-learn">Scikit-learn</a>,a package for Machine Learning in python offers packages for ensemble learning including packages for bagging and averaging methods.</li> 
       <li><a href="/wiki/MATLAB" title="MATLAB">MATLAB</a>: classification ensembles are implemented in Statistics and Machine Learning Toolbox.<sup id="cite_ref-33" class="reference"><a href="#cite_note-33">[33]</a></sup></li> 
      </ul> 
      <h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=13" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2> 
      <ul> 
       <li><a href="/wiki/Ensemble_averaging_(machine_learning)" title="Ensemble averaging (machine learning)">Ensemble averaging (machine learning)</a></li> 
       <li><a href="/wiki/Bayesian_structural_time_series" title="Bayesian structural time series">Bayesian structural time series</a> (BSTS)</li> 
      </ul> 
      <h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=14" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2> 
      <div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;"> 
       <ol class="references"> 
        <li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation journal">Opitz, D.; Maclin, R. (1999). "Popular ensemble methods: An empirical study". <i><a href="/wiki/Journal_of_Artificial_Intelligence_Research" title="Journal of Artificial Intelligence Research">Journal of Artificial Intelligence Research</a></i>. <b>11</b>: 169–198. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1613%2Fjair.614">10.1613/jair.614</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=Popular+ensemble+methods%3A+An+empirical+study&amp;rft.au=Maclin%2C+R.&amp;rft.aufirst=D.&amp;rft.aulast=Opitz&amp;rft.date=1999&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Artificial+Intelligence+Research&amp;rft.pages=169-198&amp;rft.volume=11&amp;rft_id=info%3Adoi%2F10.1613%2Fjair.614&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation journal">Polikar, R. (2006). "Ensemble based systems in decision making". <i>IEEE Circuits and Systems Magazine</i>. <b>6</b> (3): 21–45. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2FMCAS.2006.1688199">10.1109/MCAS.2006.1688199</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=Ensemble+based+systems+in+decision+making&amp;rft.aufirst=R.&amp;rft.aulast=Polikar&amp;rft.date=2006&amp;rft.genre=article&amp;rft.issue=3&amp;rft.jtitle=IEEE+Circuits+and+Systems+Magazine&amp;rft.pages=21-45&amp;rft.volume=6&amp;rft_id=info%3Adoi%2F10.1109%2FMCAS.2006.1688199&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-Rokach2010-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-Rokach2010_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Rokach2010_3-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Rokach, L. (2010). "Ensemble-based classifiers". <i>Artificial Intelligence Review</i>. <b>33</b> (1-2): 1–39. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1007%2Fs10462-009-9124-7">10.1007/s10462-009-9124-7</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=Ensemble-based+classifiers&amp;rft.aufirst=L.&amp;rft.aulast=Rokach&amp;rft.date=2010&amp;rft.genre=article&amp;rft.issue=1-2&amp;rft.jtitle=Artificial+Intelligence+Review&amp;rft.pages=1-39&amp;rft.volume=33&amp;rft_id=info%3Adoi%2F10.1007%2Fs10462-009-9124-7&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text">Kuncheva, L. and Whitaker, C., Measures of diversity in classifier ensembles, <i>Machine Learning</i>, 51, pp. 181-207, 2003</span></li> 
        <li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">Sollich, P. and Krogh, A., <i>Learning with ensembles: How overfitting can be useful</i>, Advances in Neural Information Processing Systems, volume 8, pp. 190-196, 1996.</span></li> 
        <li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text">Brown, G. and Wyatt, J. and Harris, R. and Yao, X., Diversity creation methods: a survey and categorisation., <i>Information Fusion</i>, 6(1), pp.5-20, 2005.</span></li> 
        <li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><i><a rel="nofollow" class="external text" href="http://www.clei.cl/cleiej/papers/v8i2p1.pdf">Accuracy and Diversity in Ensembles of Text Categorisers</a></i>. J. J. García Adeva, Ulises Cerviño, and R. Calvo, CLEI Journal, Vol. 8, No. 2, pp. 1 - 12, December 2005.</span></li> 
        <li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text">Ho, T., Random Decision Forests, <i>Proceedings of the Third International Conference on Document Analysis and Recognition</i>, pp. 278-282, 1995.</span></li> 
        <li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">Gashler, M. and Giraud-Carrier, C. and Martinez, T., <i><a rel="nofollow" class="external text" href="http://axon.cs.byu.edu/papers/gashler2008icmla.pdf">Decision Tree Ensemble: Small Heterogeneous Is Better Than Large Homogeneous</a></i>, The Seventh International Conference on Machine Learning and Applications, 2008, pp. 900-905., <a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=4796917">DOI 10.1109/ICMLA.2008.154</a></span></li> 
        <li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation conference">R. Bonab, Hamed; Can, Fazli (2016). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=2983907"><i>A Theoretical Framework on the Ideal Number of Classifiers for Online Ensembles in Data Streams</i></a>. CIKM. USA: ACM. p.&nbsp;2053.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.au=Can%2C+Fazli&amp;rft.aufirst=Hamed&amp;rft.aulast=R.+Bonab&amp;rft.btitle=A+Theoretical+Framework+on+the+Ideal+Number+of+Classifiers+for+Online+Ensembles+in+Data+Streams&amp;rft.date=2016&amp;rft.genre=conference&amp;rft.pages=2053&amp;rft.place=USA&amp;rft.pub=ACM&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D2983907&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><a href="/wiki/Tom_M._Mitchell" title="Tom M. Mitchell">Tom M. Mitchell</a>, <i>Machine Learning</i>, 1997, pp. 175</span></li> 
        <li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text">Breiman, L., Bagging Predictors, <i>Machine Learning</i>, 24(2), pp.123-140, 1996.</span></li> 
        <li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hoeting, J. A.; Madigan, D.; Raftery, A. E.; Volinsky, C. T. (1999). "Bayesian Model Averaging: A Tutorial". <i>Statistical Science</i>. <b>14</b> (4): 382–401. <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a>&nbsp;<a rel="nofollow" class="external text" href="//www.jstor.org/stable/2676803">2676803</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.2307%2F2676803">10.2307/2676803</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=Bayesian+Model+Averaging%3A+A+Tutorial&amp;rft.au=Madigan%2C+D.&amp;rft.au=Raftery%2C+A.+E.&amp;rft.au=Volinsky%2C+C.+T.&amp;rft.aufirst=J.+A.&amp;rft.aulast=Hoeting&amp;rft.date=1999&amp;rft.genre=article&amp;rft.issue=4&amp;rft.jtitle=Statistical+Science&amp;rft.pages=382-401&amp;rft.volume=14&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2676803&amp;rft_id=info%3Adoi%2F10.2307%2F2676803&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text">David Haussler, Michael Kearns, and Robert E. Schapire. <i>Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension</i>. Machine Learning, 14:83–113, 1994</span></li> 
        <li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation conference">Domingos, Pedro (2000). <a rel="nofollow" class="external text" href="http://www.cs.washington.edu/homes/pedrod/papers/mlc00b.pdf"><i>Bayesian averaging of classifiers and the overfitting problem</i></a> <span style="font-size:85%;">(PDF)</span>. Proceedings of the 17th <a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">International Conference on Machine Learning (ICML)</a>. pp.&nbsp;223––230.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.aufirst=Pedro&amp;rft.aulast=Domingos&amp;rft.btitle=Bayesian+averaging+of+classifiers+and+the+overfitting+problem&amp;rft.date=2000&amp;rft.genre=conference&amp;rft.pages=223--230&amp;rft_id=http%3A%2F%2Fwww.cs.washington.edu%2Fhomes%2Fpedrod%2Fpapers%2Fmlc00b.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite id="CITEREFMinka2002" class="citation">Minka, Thomas (2002), <a rel="nofollow" class="external text" href="http://research.microsoft.com/en-us/um/people/minka/papers/minka-bma-isnt-mc.pdf"><i>Bayesian model averaging is not model combination</i></a> <span style="font-size:85%;">(PDF)</span></cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.aufirst=Thomas&amp;rft.aulast=Minka&amp;rft.btitle=Bayesian+model+averaging+is+not+model+combination&amp;rft.date=2002&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fminka%2Fpapers%2Fminka-bma-isnt-mc.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><cite class="citation journal">Castillo, I.; Schmidt-Hieber, J.; van der Vaart, A. (2015). "Bayesian linear regression with sparse priors". <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>43</b> (5): 1986–2018. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1214%2F15-AOS1334">10.1214/15-AOS1334</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=Bayesian+linear+regression+with+sparse+priors&amp;rft.au=Schmidt-Hieber%2C+J.&amp;rft.au=van+der+Vaart%2C+A.&amp;rft.aufirst=I.&amp;rft.aulast=Castillo&amp;rft.date=2015&amp;rft.genre=article&amp;rft.issue=5&amp;rft.jtitle=Annals+of+Statistics&amp;rft.pages=1986-2018&amp;rft.volume=43&amp;rft_id=info%3Adoi%2F10.1214%2F15-AOS1334&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hernández-Lobato, D.; Hernández-Lobato, J. M.; Dupont, P. (2013). <a rel="nofollow" class="external text" href="http://www.jmlr.org/papers/volume14/hernandez-lobato13a/hernandez-lobato13a.pdf">"Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation"</a> <span style="font-size:85%;">(PDF)</span>. <i>Journal of Machine Learning Research</i>. <b>14</b>: 1891–1945.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=Generalized+Spike-and-Slab+Priors+for+Bayesian+Group+Feature+Selection+Using+Expectation+Propagation&amp;rft.au=Dupont%2C+P.&amp;rft.au=Hern%C3%A1ndez-Lobato%2C+J.+M.&amp;rft.aufirst=D.&amp;rft.aulast=Hern%C3%A1ndez-Lobato&amp;rft.date=2013&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.pages=1891-1945&amp;rft.volume=14&amp;rft_id=http%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fvolume14%2Fhernandez-lobato13a%2Fhernandez-lobato13a.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation conference">Monteith, Kristine; Carroll, James; Seppi, Kevin; Martinez, Tony. (2011). <a rel="nofollow" class="external text" href="http://axon.cs.byu.edu/papers/Kristine.ijcnn2011.pdf"><i>Turning Bayesian Model Averaging into Bayesian Model Combination</i></a> <span style="font-size:85%;">(PDF)</span>. Proceedings of the International Joint Conference on Neural Networks IJCNN'11. pp.&nbsp;2657–2663.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.au=Carroll%2C+James&amp;rft.au=Martinez%2C+Tony.&amp;rft.au=Monteith%2C+Kristine&amp;rft.au=Seppi%2C+Kevin&amp;rft.btitle=Turning+Bayesian+Model+Averaging+into+Bayesian+Model+Combination&amp;rft.date=2011&amp;rft.genre=conference&amp;rft.pages=2657-2663&amp;rft_id=http%3A%2F%2Faxon.cs.byu.edu%2Fpapers%2FKristine.ijcnn2011.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text">Saso Dzeroski, Bernard Zenko, <i><a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.6096">Is Combining Classifiers Better than Selecting the Best One</a></i>, Machine Learning, 2004, pp. 255--273</span></li> 
        <li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text">Bensusan, Hilan and Giraud-Carrier, Christophe G., Discovering Task Neighbourhoods Through Landmark Learning Performances, PKDD '00: Proceedings of the 4th European Conference on Principles of Data Mining and Knowledge Discovery, Springer-Verlag, 2000, pages 325--330</span></li> 
        <li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text">Wolpert, D., <i>Stacked Generalization.</i>, Neural Networks, 5(2), pp. 241-259., 1992</span></li> 
        <li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text">Breiman, L., <i>Stacked Regression</i>, Machine Learning, 24, 1996 <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2FBF00117832">10.1007/BF00117832</a></span></li> 
        <li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><cite class="citation journal">Ozay, M.; Yarman Vural, F. T. (2013). "A New Fuzzy Stacked Generalization Technique and Analysis of its Performance". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1204.0171">1204.0171</a>?<img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813"></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=A+New+Fuzzy+Stacked+Generalization+Technique+and+Analysis+of+its+Performance&amp;rft.au=Yarman+Vural%2C+F.+T.&amp;rft.aufirst=M.&amp;rft.aulast=Ozay&amp;rft.date=2013&amp;rft.genre=article&amp;rft_id=info%3Aarxiv%2F1204.0171&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text">Smyth, P. and Wolpert, D. H., <i>Linearly Combining Density Estimators via Stacking</i>, Machine Learning Journal, 36, 59-83, 1999</span></li> 
        <li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text">Wolpert, D.H., and Macready, W.G., <i>An Efficient Method to Estimate Bagging’s Generalization Error</i>, Machine Learning Journal, 35, 41-55, 1999</span></li> 
        <li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text">Clarke, B., <i>Bayes model averaging and stacking when model approximation error cannot be ignored</i>, Journal of Machine Learning Research, pp 683-712, 2003</span></li> 
        <li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><cite class="citation journal">Sill, J.; Takacs, G.; Mackey, L.; Lin, D. (2009). "Feature-Weighted Linear Stacking". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/0911.0460">0911.0460</a>?<img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813"></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=Feature-Weighted+Linear+Stacking&amp;rft.au=Lin%2C+D.&amp;rft.au=Mackey%2C+L.&amp;rft.au=Takacs%2C+G.&amp;rft.aufirst=J.&amp;rft.aulast=Sill&amp;rft.date=2009&amp;rft.genre=article&amp;rft_id=info%3Aarxiv%2F0911.0460&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text"><cite class="citation journal">Amini, Shahram M.; Parmeter, Christopher F. (2011). <a rel="nofollow" class="external text" href="https://core.ac.uk/download/pdf/6494889.pdf">"Bayesian model averaging in R"</a> <span style="font-size:85%;">(PDF)</span>. <i>Journal of Economic and Social Measurement</i>. <b>36</b> (4): 253–287.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=Bayesian+model+averaging+in+R&amp;rft.au=Parmeter%2C+Christopher+F.&amp;rft.aufirst=Shahram+M.&amp;rft.aulast=Amini&amp;rft.date=2011&amp;rft.genre=article&amp;rft.issue=4&amp;rft.jtitle=Journal+of+Economic+and+Social+Measurement&amp;rft.pages=253-287&amp;rft.volume=36&amp;rft_id=https%3A%2F%2Fcore.ac.uk%2Fdownload%2Fpdf%2F6494889.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://cran.r-project.org/web/packages/BMS/index.html">"BMS: Bayesian Model Averaging Library"</a>. <i>The Comprehensive R Archive Network</i><span class="reference-accessdate">. Retrieved <span class="nowrap">September 9,</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=BMS%3A+Bayesian+Model+Averaging+Library&amp;rft.genre=unknown&amp;rft.jtitle=The+Comprehensive+R+Archive+Network&amp;rft_id=https%3A%2F%2Fcran.r-project.org%2Fweb%2Fpackages%2FBMS%2Findex.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://cran.r-project.org/web/packages/BAS/index.html">"BAS: Bayesian Model Averaging using Bayesian Adaptive Sampling"</a>. <i>The Comprehensive R Archive Network</i><span class="reference-accessdate">. Retrieved <span class="nowrap">September 9,</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=BAS%3A+Bayesian+Model+Averaging+using+Bayesian+Adaptive+Sampling&amp;rft.genre=unknown&amp;rft.jtitle=The+Comprehensive+R+Archive+Network&amp;rft_id=https%3A%2F%2Fcran.r-project.org%2Fweb%2Fpackages%2FBAS%2Findex.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://cran.r-project.org/web/packages/BMA/index.html">"BMA: Bayesian Model Averaging"</a>. <i>The Comprehensive R Archive Network</i><span class="reference-accessdate">. Retrieved <span class="nowrap">September 9,</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=BMA%3A+Bayesian+Model+Averaging&amp;rft.genre=unknown&amp;rft.jtitle=The+Comprehensive+R+Archive+Network&amp;rft_id=https%3A%2F%2Fcran.r-project.org%2Fweb%2Fpackages%2FBMA%2Findex.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
        <li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://uk.mathworks.com/help/stats/classification-ensembles.html">"Classification Ensembles"</a>. <i>MATLAB &amp; Simulink</i><span class="reference-accessdate">. Retrieved <span class="nowrap">June 8,</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=Classification+Ensembles&amp;rft.genre=unknown&amp;rft.jtitle=MATLAB+%26+Simulink&amp;rft_id=https%3A%2F%2Fuk.mathworks.com%2Fhelp%2Fstats%2Fclassification-ensembles.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li> 
       </ol> 
      </div> 
      <h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=15" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2> 
      <ul> 
       <li><cite class="citation book"><a href="/wiki/Zhou_Zhihua" class="mw-redirect" title="Zhou Zhihua">Zhou Zhihua</a> (2012). <i>Ensemble Methods: Foundations and Algorithms</i>. Chapman and Hall/CRC. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/978-1-439-83003-1" title="Special:BookSources/978-1-439-83003-1">978-1-439-83003-1</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.au=Zhou+Zhihua&amp;rft.btitle=Ensemble+Methods%3A+Foundations+and+Algorithms&amp;rft.date=2012&amp;rft.genre=book&amp;rft.isbn=978-1-439-83003-1&amp;rft.pub=Chapman+and+Hall%2FCRC&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></li> 
       <li><cite class="citation book"><a href="/wiki/Robert_Schapire" title="Robert Schapire">Robert Schapire</a>; <a href="/wiki/Yoav_Freund" title="Yoav Freund">Yoav Freund</a> (2012). <i>Boosting: Foundations and Algorithms</i>. MIT. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="/wiki/Special:BookSources/978-0-262-01718-3" title="Special:BookSources/978-0-262-01718-3">978-0-262-01718-3</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.au=Robert+Schapire&amp;rft.au=Yoav+Freund&amp;rft.btitle=Boosting%3A+Foundations+and+Algorithms&amp;rft.date=2012&amp;rft.genre=book&amp;rft.isbn=978-0-262-01718-3&amp;rft.pub=MIT&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></li> 
      </ul> 
      <h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit&amp;section=16" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2> 
      <ul> 
       <li><cite class="citation web">Robi Polikar (ed.). <a rel="nofollow" class="external text" href="http://www.scholarpedia.org/article/Ensemble_learning">"Ensemble learning"</a>. <i><a href="/wiki/Scholarpedia" title="Scholarpedia">Scholarpedia</a></i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEnsemble+learning&amp;rft.atitle=Ensemble+learning&amp;rft.genre=unknown&amp;rft.jtitle=Scholarpedia&amp;rft_id=http%3A%2F%2Fwww.scholarpedia.org%2Farticle%2FEnsemble_learning&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&nbsp;</span></span></li> 
       <li>The <a href="/wiki/Waffles_(machine_learning)" title="Waffles (machine learning)">Waffles (machine learning)</a> toolkit contains implementations of Bagging, Boosting, Bayesian Model Averaging, Bayesian Model Combination, Bucket-of-models, and other ensemble techniques</li> 
      </ul> 
      <!-- 
NewPP limit report
Parsed by mw1245
Cached time: 20170922163353
Cache expiry: 1900800
Dynamic content: false
CPU time usage: 0.252 seconds
Real time usage: 0.454 seconds
Preprocessor visited node count: 1476/1000000
Preprocessor generated node count: 0/1500000
Post?expand include size: 58918/2097152 bytes
Template argument size: 1418/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 1/500
Lua time usage: 0.114/10.000 seconds
Lua memory usage: 4.83 MB/50 MB
--> 
      <!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  258.694      1 -total
 51.38%  132.913      1 Template:Reflist
 24.16%   62.504      9 Template:Cite_journal
 14.52%   37.575      1 Template:Citation_needed
 12.77%   33.034      1 Template:Fix
 11.32%   29.289      1 Template:Machine_learning_bar
 10.32%   26.705      1 Template:Sidebar_with_collapsible_lists
  8.63%   22.323      1 Template:For
  8.00%   20.699      2 Template:Category_handler
  6.19%   16.017      5 Template:Cite_web
--> 
     </div> 
     <!-- Saved in parser cache with key enwiki:pcache:idhash:22212276-0!canonical!math=5 and timestamp 20170922163353 and revision id 793794298
 --> 
     <noscript>
      <img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;">
     </noscript>
    </div> 
    <div class="printfooter">
      Retrieved from "
     <a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Ensemble_learning&amp;oldid=793794298">https://en.wikipedia.org/w/index.php?title=Ensemble_learning&amp;oldid=793794298</a>" 
    </div> 
    <div id="catlinks" class="catlinks" data-mw="interface">
     <div id="mw-normal-catlinks" class="mw-normal-catlinks">
      <a href="/wiki/Help:Category" title="Help:Category">Categories</a>: 
      <ul>
       <li><a href="/wiki/Category:Ensemble_learning" title="Category:Ensemble learning">Ensemble learning</a></li>
      </ul>
     </div>
     <div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">
      Hidden categories: 
      <ul>
       <li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li>
       <li><a href="/wiki/Category:Articles_with_unsourced_statements_from_January_2012" title="Category:Articles with unsourced statements from January 2012">Articles with unsourced statements from January 2012</a></li>
      </ul>
     </div>
    </div> 
    <div class="visualClear"></div> 
   </div> 
  </div> 
  <div id="mw-navigation"> 
   <h2>Navigation menu</h2> 
   <div id="mw-head"> 
    <div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label"> 
     <h3 id="p-personal-label">Personal tools</h3> 
     <ul> 
      <li id="pt-anonuserpage">Not logged in</li>
      <li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li>
      <li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li>
      <li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Ensemble+learning" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li>
      <li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Ensemble+learning" title="You're encouraged to log in; however, it's not mandatory. [o]" accesskey="o">Log in</a></li> 
     </ul> 
    </div> 
    <div id="left-navigation"> 
     <div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label"> 
      <h3 id="p-namespaces-label">Namespaces</h3> 
      <ul> 
       <li id="ca-nstab-main" class="selected"><span><a href="/wiki/Ensemble_learning" title="View the content page [c]" accesskey="c">Article</a></span></li> 
       <li id="ca-talk"><span><a href="/wiki/Talk:Ensemble_learning" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li> 
      </ul> 
     </div> 
     <div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label"> 
      <h3 id="p-variants-label"> <span>Variants</span> </h3> 
      <div class="menu"> 
       <ul> 
       </ul> 
      </div> 
     </div> 
    </div> 
    <div id="right-navigation"> 
     <div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label"> 
      <h3 id="p-views-label">Views</h3> 
      <ul> 
       <li id="ca-view" class="collapsible selected"><span><a href="/wiki/Ensemble_learning">Read</a></span></li> 
       <li id="ca-edit" class="collapsible"><span><a href="/w/index.php?title=Ensemble_learning&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></span></li> 
       <li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Ensemble_learning&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li> 
      </ul> 
     </div> 
     <div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label"> 
      <h3 id="p-cactions-label"><span>More</span></h3> 
      <div class="menu"> 
       <ul> 
       </ul> 
      </div> 
     </div> 
     <div id="p-search" role="search"> 
      <h3> <label for="searchInput">Search</label> </h3> 
      <form action="/w/index.php" id="searchform"> 
       <div id="simpleSearch"> 
        <input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput">
        <input type="hidden" value="Special:Search" name="title">
        <input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton">
        <input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"> 
       </div> 
      </form> 
     </div> 
    </div> 
   </div> 
   <div id="mw-panel"> 
    <div id="p-logo" role="banner">
     <a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a>
    </div> 
    <div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label"> 
     <h3 id="p-navigation-label">Navigation</h3> 
     <div class="body"> 
      <ul> 
       <li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
       <li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li>
       <li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li>
       <li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li>
       <li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li>
       <li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li>
       <li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li> 
      </ul> 
     </div> 
    </div> 
    <div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label"> 
     <h3 id="p-interaction-label">Interaction</h3> 
     <div class="body"> 
      <ul> 
       <li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li>
       <li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li>
       <li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li>
       <li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
       <li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li> 
      </ul> 
     </div> 
    </div> 
    <div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label"> 
     <h3 id="p-tb-label">Tools</h3> 
     <div class="body"> 
      <ul> 
       <li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Ensemble_learning" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li>
       <li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Ensemble_learning" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
       <li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li>
       <li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li>
       <li id="t-permalink"><a href="/w/index.php?title=Ensemble_learning&amp;oldid=793794298" title="Permanent link to this revision of the page">Permanent link</a></li>
       <li id="t-info"><a href="/w/index.php?title=Ensemble_learning&amp;action=info" title="More information about this page">Page information</a></li>
       <li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q245652" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li>
       <li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Ensemble_learning&amp;id=793794298" title="Information on how to cite this page">Cite this page</a></li> 
      </ul> 
     </div> 
    </div> 
    <div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label"> 
     <h3 id="p-coll-print_export-label">Print/export</h3> 
     <div class="body"> 
      <ul> 
       <li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Ensemble+learning">Create a book</a></li>
       <li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Ensemble+learning&amp;action=show-download-screen">Download as PDF</a></li>
       <li id="t-print"><a href="/w/index.php?title=Ensemble_learning&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li> 
      </ul> 
     </div> 
    </div> 
    <div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label"> 
     <h3 id="p-lang-label">Languages</h3> 
     <div class="body"> 
      <ul> 
       <li class="interlanguage-link interwiki-de"><a href="https://de.wikipedia.org/wiki/Ensemble_learning" title="Ensemble learning – German" lang="de" hreflang="de" class="interlanguage-link-target">Deutsch</a></li>
       <li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/Apprentissage_ensembliste" title="Apprentissage ensembliste – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li>
       <li class="interlanguage-link interwiki-ko"><a href="https://ko.wikipedia.org/wiki/%EC%95%99%EC%83%81%EB%B8%94_%ED%95%99%EC%8A%B5%EB%B2%95" title="??? ??? – Korean" lang="ko" hreflang="ko" class="interlanguage-link-target">???</a></li>
       <li class="interlanguage-link interwiki-it"><a href="https://it.wikipedia.org/wiki/Apprendimento_ensemble" title="Apprendimento ensemble – Italian" lang="it" hreflang="it" class="interlanguage-link-target">Italiano</a></li> 
      </ul> 
      <div class="after-portlet after-portlet-lang">
       <span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q245652#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span>
      </div> 
     </div> 
    </div> 
   </div> 
  </div> 
  <div id="footer" role="contentinfo"> 
   <ul id="footer-info"> 
    <li id="footer-info-lastmod"> This page was last edited on 4 August 2017, at 00:31.</li> 
    <li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>; additional terms may apply. By using this site, you agree to the <a href="//wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li> 
   </ul> 
   <ul id="footer-places"> 
    <li id="footer-places-privacy"><a href="https://wikimediafoundation.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li> 
    <li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li> 
    <li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li> 
    <li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li> 
    <li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li> 
    <li id="footer-places-cookiestatement"><a href="https://wikimediafoundation.org/wiki/Cookie_statement">Cookie statement</a></li> 
    <li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Ensemble_learning&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li> 
   </ul> 
   <ul id="footer-icons" class="noprint"> 
    <li id="footer-copyrightico"> <a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"></a> </li> 
    <li id="footer-poweredbyico"> <a href="//www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"></a> </li> 
   </ul> 
   <div style="clear:both"></div> 
  </div> 
  <script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.252","walltime":"0.454","ppvisitednodes":{"value":1476,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":58918,"limit":2097152},"templateargumentsize":{"value":1418,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":1,"limit":500},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  258.694      1 -total"," 51.38%  132.913      1 Template:Reflist"," 24.16%   62.504      9 Template:Cite_journal"," 14.52%   37.575      1 Template:Citation_needed"," 12.77%   33.034      1 Template:Fix"," 11.32%   29.289      1 Template:Machine_learning_bar"," 10.32%   26.705      1 Template:Sidebar_with_collapsible_lists","  8.63%   22.323      1 Template:For","  8.00%   20.699      2 Template:Category_handler","  6.19%   16.017      5 Template:Cite_web"]},"scribunto":{"limitreport-timeusage":{"value":"0.114","limit":"10.000"},"limitreport-memusage":{"value":5067687,"limit":52428800}},"cachereport":{"origin":"mw1245","timestamp":"20170922163353","ttl":1900800,"transientcontent":false}}});});</script>
  <script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":65,"wgHostname":"mw1257"});});</script>   
 </body>
</html>