<!doctype html>
<html lang="en" class="chekov">
 <head>
  <script>var a=window.devsite||{};window.devsite=a;a.readyCallbacks=[];window.devsite.readyCallbacks=a.readyCallbacks;a.ready=function(b){a.readyCallbacks.push(b)};window.devsite.ready=a.ready;
</script>
  <meta charset="utf-8">
  <meta name="xsrf_token" content="4yLBM4UXhxEnXkbl8-2GLItZCtUJt-DdkMwHqboS9Pc6MTUwNzAzMjYyMzc5Njc0MA">
  <link rel="canonical" href="https://www.tensorflow.org/tutorials/word2vec">
  <link rel="alternate" href="https://www.tensorflow.org/tutorials/word2vec" hreflang="en">
  <link rel="alternate" href="https://tensorflow.google.cn/tutorials/word2vec" hreflang="en-cn">
  <link rel="alternate" href="https://www.tensorflow.org/tutorials/word2vec" hreflang="x-default">
  <link rel="shortcut icon" href="https://www.tensorflow.org/_static/772880e9e9/images/tensorflow/favicon.png">
  <link rel="apple-touch-icon" href="https://www.tensorflow.org/images/apple-touch-icon-180x180.png">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:300,400,400italic,500,500italic,700,700italic|Roboto+Mono:400,500,700|Material+Icons">
  <link rel="stylesheet" href="https://www.tensorflow.org/_static/772880e9e9/css/devsite-tensorflow-orange.css">
  <link rel="search" type="application/opensearchdescription+xml" title="TensorFlow" href="https://www.tensorflow.org/s/opensearch.xml">
  <script src="https://www.tensorflow.org/_static/772880e9e9/js/jquery-bundle.js"></script>
  <meta property="og:site_name" content="TensorFlow">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://www.tensorflow.org/tutorials/word2vec">
  <meta property="og:locale" content="en">
  <script>
    var ___gcfg = ___gcfg || {};
    ___gcfg.lang = 'en';
  </script>
  <title>Vector Representations of Words &nbsp;|&nbsp; TensorFlow</title>
  <meta property="og:title" content="Vector Representations of Words &nbsp;|&nbsp; TensorFlow">
 </head>
 <body class="
               devsite-doc-page
               
               
               
               
               " id="top_of_page">
  <div class="devsite-wrapper">
   <div class="devsite-top-section-wrapper
            ">
    <header class="devsite-top-section nocontent">
     <div class="devsite-top-logo-row-wrapper-wrapper">
      <div class="devsite-top-logo-row-wrapper">
       <div class="devsite-top-logo-row devsite-full-site-width">
        <button type="button" class="devsite-expand-section-nav devsite-header-icon-button
                                       button-flat material-icons gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Hamburger menu"></button>
        <div class="devsite-product-name-wrapper">
         <a href="https://www.tensorflow.org/" class="devsite-site-logo-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Site logo"><img src="https://www.tensorflow.org/_static/772880e9e9/images/tensorflow/lockup.png" class="devsite-site-logo" alt="TensorFlow"><span class="devsite-site-name devsite-product-name">TensorFlow</span></a>
        </div>
        <div class="devsite-header-upper-tabs">
         <nav class="devsite-doc-set-nav devsite-nav devsite-overflow-tabs-scroll-wrapper">
          <ul class="devsite-doc-set-nav-tab-list devsite-overflow-tabs-scroll">
           <li class="devsite-doc-set-nav-tab-container"><a href="https://www.tensorflow.org/install/" class="
                devsite-doc-set-nav-tab gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Tab: Install"> Install </a></li>
           <li class="devsite-doc-set-nav-tab-container"><a href="https://www.tensorflow.org/get_started/" class="devsite-doc-set-nav-active
                devsite-doc-set-nav-tab gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Tab: Develop"> Develop </a></li>
           <li class="devsite-doc-set-nav-tab-container"><a href="https://www.tensorflow.org/api_docs/" class="
                devsite-doc-set-nav-tab gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Tab: API r1.3"> API r1.3 </a></li>
           <li class="devsite-doc-set-nav-tab-container"><a href="https://www.tensorflow.org/deploy/" class="
                devsite-doc-set-nav-tab gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Tab: Deploy"> Deploy </a></li>
           <li class="devsite-doc-set-nav-tab-container"><a href="https://www.tensorflow.org/extend/" class="
                devsite-doc-set-nav-tab gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Tab: Extend"> Extend </a></li>
           <li class="devsite-doc-set-nav-tab-container"><a href="https://www.tensorflow.org/community/" class="
                devsite-doc-set-nav-tab gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Tab: Community"> Community </a></li>
           <li class="devsite-doc-set-nav-tab-container"><a href="https://www.tensorflow.org/versions/" class="
                devsite-doc-set-nav-tab gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Tab: Versions"> Versions </a></li>
           <li class="devsite-doc-set-nav-tab-container"><a href="https://www.tensorflow.org/tfrc/" class="
                devsite-doc-set-nav-tab gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Tab: TFRC"> TFRC </a></li>
          </ul>
         </nav>
        </div>
        <form class="devsite-search-form" action="https://www.tensorflow.org/s/results/" method="GET" id="top-search" search-placeholder="Search">
         <div id="searchbox" class="devsite-searchbox">
          <input placeholder="Search" type="text" class="devsite-search-field devsite-search-query" name="q" value="" autocomplete="off">
          <div class="devsite-search-image material-icons"></div>
         </div>
         <input type="hidden" name="p" id="search_project" value="/" data-project-name="TensorFlow" data-project-path="/" data-query-match="">
         <input type="hidden" class="suggest-project" value="TensorFlow">
         <div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Android Developers"></div>
         <div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Android Open Source Project"></div>
         <div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="API.AI"></div>
         <div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Google Cloud Platform"></div>
         <div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Firebase"></div>
         <div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Link.app"></div>
         <div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="Nest Developers"></div>
         <div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="OpenThread"></div>
         <div class="suggest-project-metadata" data-home_url="/" data-path="/" data-name="TensorFlow"></div>
        </form>
        <a class="devsite-header-link devsite-top-button button gc-analytics-event" href="https://github.com/tensorflow" data-category="Site-Wide Custom Events" data-label="Site header link">
         <div class="devsite-header-link-label">
          GitHub
         </div></a>
        <button type="button" class="devsite-search-button devsite-header-icon-button button-flat
                                       material-icons"></button>
       </div>
      </div>
     </div>
     <div class="devsite-collapsible-section">
      <div class="devsite-header-background devsite-full-site-width">
       <div class="devsite-product-id-row devsite-full-site-width">
        <div class="devsite-product-description-row">
         <ul class="devsite-breadcrumb-list">
          <li class="devsite-breadcrumb-item"> Develop </li>
         </ul>
        </div>
       </div>
       <div class="devsite-doc-set-nav-row devsite-full-site-width">
        <nav class="devsite-doc-set-nav devsite-nav devsite-overflow-tabs-scroll-wrapper">
         <ul class="devsite-doc-set-nav-tab-list devsite-overflow-tabs-scroll">
          <li class="devsite-doc-set-nav-tab-container"><a href="https://www.tensorflow.org/get_started/" class="
                devsite-doc-set-nav-tab gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Tab: Get Started"> Get Started </a></li>
          <li class="devsite-doc-set-nav-tab-container"><a href="https://www.tensorflow.org/programmers_guide/" class="
                devsite-doc-set-nav-tab gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Tab: Programmer's Guide"> Programmer's Guide </a></li>
          <li class="devsite-doc-set-nav-tab-container"><a href="https://www.tensorflow.org/tutorials/" class="devsite-doc-set-nav-active
                devsite-doc-set-nav-tab gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Tab: Tutorials"> Tutorials </a></li>
          <li class="devsite-doc-set-nav-tab-container"><a href="https://www.tensorflow.org/performance/" class="
                devsite-doc-set-nav-tab gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Tab: Performance"> Performance </a></li>
         </ul>
        </nav>
       </div>
      </div>
     </div>
    </header>
    <script>
    if (window.jQuery) {
      $(document).ready(function() {
        if (window.devsite && window.devsite.search) {
          
          window.devsite.search.init('en')
        }
      });
    }
  </script>
   </div>
   <div id="gc-wrapper" itemscope itemtype="http://schema.org/Article">
    <input class="google-analytics-id-json" type="hidden" value="{&quot;dimensions&quot;: {&quot;dimension6&quot;: &quot;en&quot;, &quot;dimension4&quot;: &quot;TensorFlow&quot;, &quot;dimension5&quot;: &quot;en&quot;, &quot;dimension3&quot;: false, &quot;dimension1&quot;: &quot;Signed out&quot;, &quot;dimension8&quot;: null}, &quot;gaid&quot;: &quot;UA-69864048-1&quot;}">
    <script>
      var dataLayer = [{"freeTrialEligibleUser": "False", "userCountry": "AR", "language": {"requested": "en", "served": "en"}, "projectName": "TensorFlow", "scriptsafe": null, "signedIn": "False", "internalUser": "False"}];
    </script> 
    <div class="devsite-site-mask"></div> 
    <nav class="devsite-nav-responsive devsite-nav nocontent" tabindex="0"> 
     <div class="devsite-nav-responsive-tabs-panel"> 
      <nav class="devsite-nav-responsive-tabs devsite-nav"> 
       <ul class="devsite-nav-list"> 
        <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://www.tensorflow.org/install/?nav=true" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                " data-category="Site-Wide Custom Events" data-label="Responsive Tab: Install"> Install </a> </li> 
        <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://www.tensorflow.org/get_started/?nav=true" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                devsite-nav-active" data-category="Site-Wide Custom Events" data-label="Responsive Tab: Develop"> Develop </a> 
         <nav class="devsite-nav-responsive-tabs devsite-nav"> 
          <ul class="devsite-nav-list"> 
           <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://www.tensorflow.org/get_started/?nav=true" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                " data-category="Site-Wide Custom Events" data-label="Responsive Tab: Get Started"> Get Started </a> </li> 
           <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://www.tensorflow.org/programmers_guide/?nav=true" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                " data-category="Site-Wide Custom Events" data-label="Responsive Tab: Programmer's Guide"> Programmer's Guide </a> </li> 
           <li class="devsite-nav-item devsite-nav-item-heading"> <span class="devsite-nav-responsive-forward devsite-nav-responsive-tab devsite-nav-title
                   devsite-nav-active gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Responsive Tab: Tutorials" tabindex="0"> Tutorials </span> </li> 
           <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://www.tensorflow.org/performance/?nav=true" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                " data-category="Site-Wide Custom Events" data-label="Responsive Tab: Performance"> Performance </a> </li> 
           <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://github.com/tensorflow" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Responsive Tab: GitHub"> GitHub </a> </li> 
          </ul> 
         </nav> </li> 
        <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://www.tensorflow.org/api_docs/?nav=true" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                " data-category="Site-Wide Custom Events" data-label="Responsive Tab: API r1.3"> API r1.3 </a> </li> 
        <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://www.tensorflow.org/deploy/?nav=true" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                " data-category="Site-Wide Custom Events" data-label="Responsive Tab: Deploy"> Deploy </a> </li> 
        <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://www.tensorflow.org/extend/?nav=true" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                " data-category="Site-Wide Custom Events" data-label="Responsive Tab: Extend"> Extend </a> </li> 
        <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://www.tensorflow.org/community/?nav=true" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                " data-category="Site-Wide Custom Events" data-label="Responsive Tab: Community"> Community </a> </li> 
        <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://www.tensorflow.org/versions/?nav=true" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                " data-category="Site-Wide Custom Events" data-label="Responsive Tab: Versions"> Versions </a> </li> 
        <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://www.tensorflow.org/tfrc/" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event
                " data-category="Site-Wide Custom Events" data-label="Responsive Tab: TFRC"> TFRC </a> </li> 
        <li class="devsite-nav-item devsite-nav-item-heading"> <a href="https://github.com/tensorflow" class="devsite-nav-responsive-tab devsite-nav-title gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Responsive Tab: GitHub"> GitHub </a> </li> 
       </ul> 
      </nav> 
     </div> 
     <div class="devsite-nav-responsive-sidebar-panel"> 
      <div class="devsite-nav-responsive-back" tabindex="0"></div> 
      <nav class="devsite-nav-responsive-sidebar"> 
       <ul class="devsite-nav-list">
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Tutorials</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/using_gpu" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Using GPUs</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/image_recognition" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Image Recognition</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/image_retraining" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">How to Retrain Inception's Final Layer for New Categories</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/layers" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">A Guide to TF Layers: Building a Convolutional Neural Network</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/deep_cnn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Convolutional Neural Networks</a></li>
        <li class="devsite-nav-item devsite-nav-active"><a href="https://www.tensorflow.org/tutorials/word2vec" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Vector Representations of Words</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/recurrent" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Recurrent Neural Networks</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/seq2seq" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Sequence-to-Sequence Models</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/linear" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Large-scale Linear Models with TensorFlow</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/wide" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Linear Model Tutorial</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/wide_and_deep" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Wide &amp; Deep Learning Tutorial</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/kernel_methods" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Improving Linear Models Using Explicit Kernel Methods</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/mandelbrot" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Mandelbrot Set</a></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/pdes" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Partial Differential Equations</a></li>
        <li class="devsite-nav-item">
         <hr class="devsite-nav-break"></li>
        <li class="devsite-nav-item"><a href="https://www.tensorflow.org/versions/" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Versions</a></li>
       </ul> 
      </nav> 
     </div> 
    </nav> 
    <div class="devsite-main-content clearfix"> 
     <nav class="devsite-section-nav devsite-nav nocontent"> 
      <ul class="devsite-nav-list">
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Tutorials</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/using_gpu" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Using GPUs</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/image_recognition" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Image Recognition</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/image_retraining" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">How to Retrain Inception's Final Layer for New Categories</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/layers" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">A Guide to TF Layers: Building a Convolutional Neural Network</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/deep_cnn" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Convolutional Neural Networks</a></li>
       <li class="devsite-nav-item devsite-nav-active"><a href="https://www.tensorflow.org/tutorials/word2vec" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Vector Representations of Words</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/recurrent" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Recurrent Neural Networks</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/seq2seq" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Sequence-to-Sequence Models</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/linear" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Large-scale Linear Models with TensorFlow</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/wide" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Linear Model Tutorial</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/wide_and_deep" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Wide &amp; Deep Learning Tutorial</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/kernel_methods" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Improving Linear Models Using Explicit Kernel Methods</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/mandelbrot" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Mandelbrot Set</a></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/tutorials/pdes" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">Partial Differential Equations</a></li>
       <li class="devsite-nav-item">
        <hr class="devsite-nav-break"></li>
       <li class="devsite-nav-item"><a href="https://www.tensorflow.org/versions/" class="devsite-nav-title gc-analytics-event" track-type="leftNav" track-name="titleLink" track-metadata-position="0" data-category="Site-Wide Custom Events" data-label="Left nav" data-action="click">TensorFlow Versions</a></li>
      </ul> 
     </nav> 
     <nav class="devsite-page-nav devsite-nav"></nav> 
     <article class="devsite-article"> 
      <article class="devsite-article-inner"> 
       <nav class="devsite-breadcrumb-nav devsite-nav"> 
        <ul class="devsite-breadcrumb-list"> 
         <li class="devsite-breadcrumb-item"> <a href="https://www.tensorflow.org/" class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="1"> TensorFlow </a> </li> 
         <li class="devsite-breadcrumb-item"> 
          <div class="devsite-breadcrumb-guillemet material-icons"></div> <a href="https://www.tensorflow.org/get_started/" class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="2"> Develop </a> </li> 
         <li class="devsite-breadcrumb-item"> 
          <div class="devsite-breadcrumb-guillemet material-icons"></div> <a href="https://www.tensorflow.org/tutorials/" class="devsite-breadcrumb-link gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Breadcrumbs" data-value="3"> Tutorials </a> </li> 
        </ul> 
       </nav> 
       <h1 itemprop="name" class="devsite-page-title"> Vector Representations of Words </h1> 
       <nav class="devsite-page-nav-embedded devsite-nav"></nav> 
       <div class="devsite-article-body clearfix
            " itemprop="articleBody"> 
        <script src="https://www.tensorflow.org/_static/772880e9e9/js/managed/mathjax/MathJax.js?config=TeX-AMS-MML_SVG"></script> 
        <!-- DO NOT EDIT! Automatically generated file. --> 
        <!-- DO NOT EDIT! Automatically generated file. --> 
        <p>In this tutorial we look at the word2vec model by <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Mikolov et al.</a> This model is used for learning vector representations of words, called "word embeddings".</p> 
        <h2 id="highlights">Highlights</h2> 
        <p>This tutorial is meant to highlight the interesting, substantive parts of building a word2vec model in TensorFlow.</p> 
        <ul> 
         <li>We start by giving the motivation for why we would want to represent words as vectors.</li> 
         <li>We look at the intuition behind the model and how it is trained (with a splash of math for good measure).</li> 
         <li>We also show a simple implementation of the model in TensorFlow.</li> 
         <li>Finally, we look at ways to make the naive version scale better.</li> 
        </ul> 
        <p>We walk through the code later during the tutorial, but if you'd prefer to dive straight in, feel free to look at the minimalistic implementation in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/word2vec/word2vec_basic.py">tensorflow/examples/tutorials/word2vec/word2vec_basic.py</a> This basic example contains the code needed to download some data, train on it a bit and visualize the result. Once you get comfortable with reading and running the basic version, you can graduate to <a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/embedding/word2vec.py">models/tutorials/embedding/word2vec.py</a> which is a more serious implementation that showcases some more advanced TensorFlow principles about how to efficiently use threads to move data into a text model, how to checkpoint during training, etc.</p> 
        <p>But first, let's look at why we would want to learn word embeddings in the first place. Feel free to skip this section if you're an Embedding Pro and you'd just like to get your hands dirty with the details.</p> 
        <h2 id="motivation_why_learn_word_embeddings">Motivation: Why Learn Word Embeddings?</h2> 
        <p>Image and audio processing systems work with rich, high-dimensional datasets encoded as vectors of the individual raw pixel-intensities for image data, or e.g. power spectral density coefficients for audio data. For tasks like object or speech recognition we know that all the information required to successfully perform the task is encoded in the data (because humans can perform these tasks from the raw data). However, natural language processing systems traditionally treat words as discrete atomic symbols, and therefore 'cat' may be represented as <code>Id537</code> and 'dog' as <code>Id143</code>. These encodings are arbitrary, and provide no useful information to the system regarding the relationships that may exist between the individual symbols. This means that the model can leverage very little of what it has learned about 'cats' when it is processing data about 'dogs' (such that they are both animals, four-legged, pets, etc.). Representing words as unique, discrete ids furthermore leads to data sparsity, and usually means that we may need more data in order to successfully train statistical models. Using vector representations can overcome some of these obstacles.</p> 
        <div style="width:100%; margin:auto; margin-bottom:10px; margin-top:20px;"> 
         <img style="width:100%" src="https://www.tensorflow.org/images/audio-image-text.png" alt> 
        </div> 
        <p><a href="https://en.wikipedia.org/wiki/Vector_space_model">Vector space models</a> (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other'). VSMs have a long, rich history in NLP, but all methods depend in some way or another on the <a href="https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis">Distributional Hypothesis</a>, which states that words that appear in the same contexts share semantic meaning. The different approaches that leverage this principle can be divided into two categories: <em>count-based methods</em> (e.g. <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">Latent Semantic Analysis</a>), and <em>predictive methods</em> (e.g. <a href="http://www.scholarpedia.org/article/Neural_net_language_models">neural probabilistic language models</a>).</p> 
        <p>This distinction is elaborated in much more detail by <a href="http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf">Baroni et al.</a>, but in a nutshell: Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word. Predictive models directly try to predict a word from its neighbors in terms of learned small, dense <em>embedding vectors</em> (considered parameters of the model).</p> 
        <p>Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text. It comes in two flavors, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model (Section 3.1 and 3.2 in <a href="http://arxiv.org/pdf/1301.3781.pdf">Mikolov et al.</a>). Algorithmically, these models are similar, except that CBOW predicts target words (e.g. 'mat') from source context words ('the cat sits on the'), while the skip-gram does the inverse and predicts source context-words from the target words. This inversion might seem like an arbitrary choice, but statistically it has the effect that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets. We will focus on the skip-gram model in the rest of this tutorial.</p> 
        <h2 id="scaling_up_with_noise-contrastive_training">Scaling up with Noise-Contrastive Training</h2> 
        <p>Neural probabilistic language models are traditionally trained using the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood">maximum likelihood</a> (ML) principle to maximize the probability of the next word \(w_t\) (for "target") given the previous words \(h\) (for "history") in terms of a <a href="https://en.wikipedia.org/wiki/Softmax_function"><em>softmax</em> function</a>,</p> 
        <div>
          $$ \begin{align} P(w_t | h) &amp;= \text{softmax} (\text{score} (w_t, h)) \\ &amp;= \frac{\exp \{ \text{score} (w_t, h) \} } {\sum_\text{Word w' in Vocab} \exp \{ \text{score} (w', h) \} } \end{align} $$
        </div> 
        <p>where \(\text{score} (w_t, h)\) computes the compatibility of word \(w_t\) with the context \(h\) (a dot product is commonly used). We train this model by maximizing its <a href="https://en.wikipedia.org/wiki/Likelihood_function">log-likelihood</a> on the training set, i.e. by maximizing</p> 
        <div>
          $$ \begin{align} J_\text{ML} &amp;= \log P(w_t | h) \\ &amp;= \text{score} (w_t, h) - \log \left( \sum_\text{Word w' in Vocab} \exp \{ \text{score} (w', h) \} \right). \end{align} $$
        </div> 
        <p>This yields a properly normalized probabilistic model for language modeling. However this is very expensive, because we need to compute and normalize each probability using the score for all other \(V\) words \(w'\) in the current context \(h\), <em>at every training step</em>.</p> 
        <div style="width:60%; margin:auto; margin-bottom:10px; margin-top:20px;"> 
         <img style="width:100%" src="https://www.tensorflow.org/images/softmax-nplm.png" alt> 
        </div> 
        <p>On the other hand, for feature learning in word2vec we do not need a full probabilistic model. The CBOW and skip-gram models are instead trained using a binary classification objective (<a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>) to discriminate the real target words \(w_t\) from \(k\) imaginary (noise) words \(\tilde w\), in the same context. We illustrate this below for a CBOW model. For skip-gram the direction is simply inverted.</p> 
        <div style="width:60%; margin:auto; margin-bottom:10px; margin-top:20px;"> 
         <img style="width:100%" src="https://www.tensorflow.org/images/nce-nplm.png" alt> 
        </div> 
        <p>Mathematically, the objective (for each example) is to maximize</p> 
        <div>
          $$J_\text{NEG} = \log Q_\theta(D=1 |w_t, h) + k \mathop{\mathbb{E}}_{\tilde w \sim P_\text{noise}} \left[ \log Q_\theta(D = 0 |\tilde w, h) \right]$$
        </div> 
        <p>where \(Q_\theta(D=1 | w, h)\) is the binary logistic regression probability under the model of seeing the word \(w\) in the context \(h\) in the dataset \(D\), calculated in terms of the learned embedding vectors \(\theta\). In practice we approximate the expectation by drawing \(k\) contrastive words from the noise distribution (i.e. we compute a <a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration">Monte Carlo average</a>).</p> 
        <p>This objective is maximized when the model assigns high probabilities to the real words, and low probabilities to noise words. Technically, this is called <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Negative Sampling</a>, and there is good mathematical motivation for using this loss function: The updates it proposes approximate the updates of the softmax function in the limit. But computationally it is especially appealing because computing the loss function now scales only with the number of <em>noise words</em> that we select (\(k\)), and not <em>all words</em> in the vocabulary (\(V\)). This makes it much faster to train. We will actually make use of the very similar <a href="http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf">noise-contrastive estimation (NCE)</a> loss, for which TensorFlow has a handy helper function <code>tf.nn.nce_loss()</code>.</p> 
        <p>Let's get an intuitive feel for how this would work in practice!</p> 
        <h2 id="the_skip-gram_model">The Skip-gram Model</h2> 
        <p>As an example, let's consider the dataset</p> 
        <p><code>the quick brown fox jumped over the lazy dog</code></p> 
        <p>We first form a dataset of words and the contexts in which they appear. We could define 'context' in any way that makes sense, and in fact people have looked at syntactic contexts (i.e. the syntactic dependents of the current target word, see e.g. <a href="https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf">Levy et al.</a>), words-to-the-left of the target, words-to-the-right of the target, etc. For now, let's stick to the vanilla definition and define 'context' as the window of words to the left and to the right of a target word. Using a window size of 1, we then have the dataset</p> 
        <p><code>([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...</code></p> 
        <p>of <code>(context, target)</code> pairs. Recall that skip-gram inverts contexts and targets, and tries to predict each context word from its target word, so the task becomes to predict 'the' and 'brown' from 'quick', 'quick' and 'fox' from 'brown', etc. Therefore our dataset becomes</p> 
        <p><code>(quick, the), (quick, brown), (brown, quick), (brown, fox), ...</code></p> 
        <p>of <code>(input, output)</code> pairs. The objective function is defined over the entire dataset, but we typically optimize this with <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a> (SGD) using one example at a time (or a 'minibatch' of <code>batch_size</code> examples, where typically <code>16 &lt;= batch_size &lt;= 512</code>). So let's look at one step of this process.</p> 
        <p>Let's imagine at training step \(t\) we observe the first training case above, where the goal is to predict <code>the</code> from <code>quick</code>. We select <code>num_noise</code> number of noisy (contrastive) examples by drawing from some noise distribution, typically the unigram distribution, \(P(w)\). For simplicity let's say <code>num_noise=1</code> and we select <code>sheep</code> as a noisy example. Next we compute the loss for this pair of observed and noisy examples, i.e. the objective at time step \(t\) becomes</p> 
        <div>
          $$J^{(t)}_\text{NEG} = \log Q_\theta(D=1 | \text{the, quick}) + \log(Q_\theta(D=0 | \text{sheep, quick}))$$
        </div> 
        <p>The goal is to make an update to the embedding parameters \(\theta\) to improve (in this case, maximize) this objective function. We do this by deriving the gradient of the loss with respect to the embedding parameters \(\theta\), i.e. \(\frac{\partial}{\partial \theta} J_\text{NEG}\) (luckily TensorFlow provides easy helper functions for doing this!). We then perform an update to the embeddings by taking a small step in the direction of the gradient. When this process is repeated over the entire training set, this has the effect of 'moving' the embedding vectors around for each word until the model is successful at discriminating real words from noise words.</p> 
        <p>We can visualize the learned vectors by projecting them down to 2 dimensions using for instance something like the <a href="http://lvdmaaten.github.io/tsne/">t-SNE dimensionality reduction technique</a>. When we inspect these visualizations it becomes apparent that the vectors capture some general, and in fact quite useful, semantic information about words and their relationships to one another. It was very interesting when we first discovered that certain directions in the induced vector space specialize towards certain semantic relationships, e.g. <em>male-female</em>, <em>verb tense</em> and even <em>country-capital</em> relationships between words, as illustrated in the figure below (see also for example <a href="http://www.aclweb.org/anthology/N13-1090">Mikolov et al., 2013</a>).</p> 
        <div style="width:100%; margin:auto; margin-bottom:10px; margin-top:20px;"> 
         <img style="width:100%" src="https://www.tensorflow.org/images/linear-relationships.png" alt> 
        </div> 
        <p>This explains why these vectors are also useful as features for many canonical NLP prediction tasks, such as part-of-speech tagging or named entity recognition (see for example the original work by <a href="http://arxiv.org/abs/1103.0398">Collobert et al., 2011</a> (<a href="http://arxiv.org/pdf/1103.0398.pdf">pdf</a>), or follow-up work by <a href="http://www.aclweb.org/anthology/P10-1040">Turian et al., 2010</a>).</p> 
        <p>But for now, let's just use them to draw pretty pictures!</p> 
        <h2 id="building_the_graph">Building the Graph</h2> 
        <p>This is all about embeddings, so let's define our embedding matrix. This is just a big random matrix to start. We'll initialize the values to be uniform in the unit cube.</p> 
        <pre class="prettyprint lang-python"><code>embeddings = tf.Variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
</code></pre> 
        <p>The noise-contrastive estimation loss is defined in terms of a logistic regression model. For this, we need to define the weights and biases for each word in the vocabulary (also called the <code>output weights</code> as opposed to the <code>input embeddings</code>). So let's define that.</p> 
        <pre class="prettyprint lang-python"><code>nce_weights = tf.Variable(
  tf.truncated_normal([vocabulary_size, embedding_size],
                      stddev=1.0 / math.sqrt(embedding_size)))
nce_biases = tf.Variable(tf.zeros([vocabulary_size]))
</code></pre> 
        <p>Now that we have the parameters in place, we can define our skip-gram model graph. For simplicity, let's suppose we've already integerized our text corpus with a vocabulary so that each word is represented as an integer (see <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/word2vec/word2vec_basic.py">tensorflow/examples/tutorials/word2vec/word2vec_basic.py</a> for the details). The skip-gram model takes two inputs. One is a batch full of integers representing the source context words, the other is for the target words. Let's create placeholder nodes for these inputs, so that we can feed in data later.</p> 
        <pre class="prettyprint lang-python"><code># Placeholders for inputs
train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
</code></pre> 
        <p>Now what we need to do is look up the vector for each of the source words in the batch. TensorFlow has handy helpers that make this easy.</p> 
        <pre class="prettyprint lang-python"><code>embed = tf.nn.embedding_lookup(embeddings, train_inputs)
</code></pre> 
        <p>Ok, now that we have the embeddings for each word, we'd like to try to predict the target word using the noise-contrastive training objective.</p> 
        <pre class="prettyprint lang-python"><code># Compute the NCE loss, using a sample of the negative labels each time.
loss = tf.reduce_mean(
  tf.nn.nce_loss(weights=nce_weights,
                 biases=nce_biases,
                 labels=train_labels,
                 inputs=embed,
                 num_sampled=num_sampled,
                 num_classes=vocabulary_size))
</code></pre> 
        <p>Now that we have a loss node, we need to add the nodes required to compute gradients and update the parameters, etc. For this we will use stochastic gradient descent, and TensorFlow has handy helpers to make this easy as well.</p> 
        <pre class="prettyprint lang-python"><code># We use the SGD optimizer.
optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)
</code></pre> 
        <h2 id="training_the_model">Training the Model</h2> 
        <p>Training the model is then as simple as using a <code>feed_dict</code> to push data into the placeholders and calling <a href="https://www.tensorflow.org/api_docs/python/tf/Session#run"><code>tf.Session.run</code></a> with this new data in a loop.</p> 
        <pre class="prettyprint lang-python"><code>for inputs, labels in generate_batch(...):
  feed_dict = {train_inputs: inputs, train_labels: labels}
  _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)
</code></pre> 
        <p>See the full example code in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/word2vec/word2vec_basic.py">tensorflow/examples/tutorials/word2vec/word2vec_basic.py</a>.</p> 
        <h2 id="visualizing_the_learned_embeddings">Visualizing the Learned Embeddings</h2> 
        <p>After training has finished we can visualize the learned embeddings using t-SNE.</p> 
        <div style="width:100%; margin:auto; margin-bottom:10px; margin-top:20px;"> 
         <img style="width:100%" src="https://www.tensorflow.org/images/tsne.png" alt> 
        </div> 
        <p>Et voila! As expected, words that are similar end up clustering nearby each other. For a more heavyweight implementation of word2vec that showcases more of the advanced features of TensorFlow, see the implementation in <a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/embedding/word2vec.py">models/tutorials/embedding/word2vec.py</a>.</p> 
        <h2 id="evaluating_embeddings_analogical_reasoning">Evaluating Embeddings: Analogical Reasoning</h2> 
        <p>Embeddings are useful for a wide variety of prediction tasks in NLP. Short of training a full-blown part-of-speech model or named-entity model, one simple way to evaluate embeddings is to directly use them to predict syntactic and semantic relationships like <code>king is to queen as father is to ?</code>. This is called <em>analogical reasoning</em> and the task was introduced by <a href="http://www.anthology.aclweb.org/N/N13/N13-1090.pdf">Mikolov and colleagues </a>. Download the dataset for this task from <a href="http://download.tensorflow.org/data/questions-words.txt">download.tensorflow.org</a>.</p> 
        <p>To see how we do this evaluation, have a look at the <code>build_eval_graph()</code> and <code>eval()</code> functions in <a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/embedding/word2vec.py">models/tutorials/embedding/word2vec.py</a>.</p> 
        <p>The choice of hyperparameters can strongly influence the accuracy on this task. To achieve state-of-the-art performance on this task requires training over a very large dataset, carefully tuning the hyperparameters and making use of tricks like subsampling the data, which is out of the scope of this tutorial.</p> 
        <h2 id="optimizing_the_implementation">Optimizing the Implementation</h2> 
        <p>Our vanilla implementation showcases the flexibility of TensorFlow. For example, changing the training objective is as simple as swapping out the call to <code>tf.nn.nce_loss()</code> for an off-the-shelf alternative such as <code>tf.nn.sampled_softmax_loss()</code>. If you have a new idea for a loss function, you can manually write an expression for the new objective in TensorFlow and let the optimizer compute its derivatives. This flexibility is invaluable in the exploratory phase of machine learning model development, where we are trying out several different ideas and iterating quickly.</p> 
        <p>Once you have a model structure you're satisfied with, it may be worth optimizing your implementation to run more efficiently (and cover more data in less time). For example, the naive code we used in this tutorial would suffer compromised speed because we use Python for reading and feeding data items -- each of which require very little work on the TensorFlow back-end. If you find your model is seriously bottlenecked on input data, you may want to implement a custom data reader for your problem, as described in <a href="https://www.tensorflow.org/extend/new_data_formats">New Data Formats</a>. For the case of Skip-Gram modeling, we've actually already done this for you as an example in <a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/embedding/word2vec.py">models/tutorials/embedding/word2vec.py</a>.</p> 
        <p>If your model is no longer I/O bound but you want still more performance, you can take things further by writing your own TensorFlow Ops, as described in <a href="https://www.tensorflow.org/extend/adding_an_op">Adding a New Op</a>. Again we've provided an example of this for the Skip-Gram case <a href="https://www.tensorflow.org/code/tensorflow_models/tutorials/embedding/word2vec_optimized.py">models/tutorials/embedding/word2vec_optimized.py</a>. Feel free to benchmark these against each other to measure performance improvements at each stage.</p> 
        <h2 id="conclusion">Conclusion</h2> 
        <p>In this tutorial we covered the word2vec model, a computationally efficient model for learning word embeddings. We motivated why embeddings are useful, discussed efficient training techniques and showed how to implement all of this in TensorFlow. Overall, we hope that this has show-cased how TensorFlow affords you the flexibility you need for early experimentation, and the control you later need for bespoke optimized implementation.</p> 
       </div> 
       <div class="devsite-content-footer nocontent"> 
        <p>Except as otherwise noted, the content of this page is licensed under the <a href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution 3.0 License</a>, and code samples are licensed under the <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache 2.0 License</a>. For details, see our <a href="https://developers.google.com/terms/site-policies">Site Policies</a>. Java is a registered trademark of Oracle and/or its affiliates.</p> 
        <p class="devsite-content-footer-date" itemprop="datePublished" content="2017-08-17T01:23:43.776910"> Last updated August 17, 2017. </p> 
       </div> 
      </article> 
     </article> 
    </div> 
    <footer class="devsite-footer-linkboxes nocontent
               devsite-footer-linkboxes-all-backup
               ">
     <nav class="devsite-full-site-width">
      <ul class="devsite-footer-linkboxes-list">
       <li class="devsite-footer-linkbox devsite-footer-linkbox-backup"><h3 class="devsite-footer-linkbox-heading">Stay Connected</h3>
        <ul class="devsite-footer-linkbox-list">
         <li class="devsite-footer-linkbox-item"><a href="//research.googleblog.com/search/label/TensorFlow" class="gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Footer Blog Link"> Blog </a></li>
         <li class="devsite-footer-linkbox-item"><a href="//github.com/tensorflow/" class="gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Footer GitHub Link"> GitHub </a></li>
         <li class="devsite-footer-linkbox-item"><a href="//twitter.com/tensorflow" class="gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Footer Twitter Link"> Twitter </a></li>
        </ul></li>
       <li class="devsite-footer-linkbox devsite-footer-linkbox-backup"><h3 class="devsite-footer-linkbox-heading">Support</h3>
        <ul class="devsite-footer-linkbox-list">
         <li class="devsite-footer-linkbox-item"><a href="//github.com/tensorflow/tensorflow/issues" class="gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Footer Issue Tracker Link"> Issue Tracker </a></li>
         <li class="devsite-footer-linkbox-item"><a href="//github.com/tensorflow/tensorflow/blob/master/RELEASE.md" class="gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Footer Release Notes Link"> Release Notes </a></li>
         <li class="devsite-footer-linkbox-item"><a href="//stackoverflow.com/questions/tagged/tensorflow" class="gc-analytics-event" data-category="Site-Wide Custom Events" data-label="Footer Stack Overflow Link"> Stack Overflow </a></li>
        </ul></li>
      </ul>
     </nav>
    </footer>
    <footer class="devsite-utility-footer">
     <nav class="devsite-utility-footer-nav devsite-nav devsite-full-site-width">
      <div class="devsite-utility-footer-nav-left">
       <form class="devsite-utility-footer-language" action="/i18n/setlang/" method="post">
        <input type="hidden" name="xsrf_token" value="4yLBM4UXhxEnXkbl8-2GLItZCtUJt-DdkMwHqboS9Pc6MTUwNzAzMjYyMzc5Njc0MA">
        <input type="hidden" name="next" value="/tutorials/word2vec">
        <select class="devsite-utility-footer-language-select kd-select" name="language" track-type="languageSelector" track-name="click"><option value="en" selected track-type="languageSelector" track-name="changed" track-metadata-original-language="en" track-metadata-selected-language="en"> English </option><option value="zh-cn" track-type="languageSelector" track-name="changed" track-metadata-original-language="en" track-metadata-selected-language="zh-cn"> ???? </option></select>
       </form>
       <span class="devsite-utility-footer-links"><a class="devsite-utility-footer-link gc-analytics-event" href="https://www.google.com/policies/terms/" data-category="Site-Wide Custom Events" data-label="Footer terms link" data-footer-link-id="terms">Terms </a><a class="devsite-utility-footer-link gc-analytics-event" href="https://www.google.com/policies/privacy/" data-category="Site-Wide Custom Events" data-label="Footer privacy link" data-footer-link-id="privacy" data-cookie-policy="//www.google.com/policies/technologies/cookies/">Privacy </a></span>
      </div>
     </nav>
    </footer>
   </div>
   <script async defer src="//www.gstatic.com/feedback/api.js"></script>
   <script src="https://www.tensorflow.org/_static/772880e9e9/js/jquery_ui-bundle.js"></script>
   <script src="https://www.tensorflow.org/_static2/772880e9e9/jsi18n/"></script>
   <script src="https://www.tensorflow.org/_static/772880e9e9/js/script_foot_closure.js"></script>
   <script src="https://www.tensorflow.org/_static/772880e9e9/js/script_foot.js"></script>
   <script>
        (function($) {
          
          devsite.devsite.Init($, {'FULL_SITE_SEARCH_ENABLED': 1, 'ENABLE_BLOCKED_VIDEO_PLACEHOLDER': 0, 'VERSION_HASH': '772880e9e9', 'SITE_NAME': 'tensorflow', 'HISTORY_ENABLED': 0, 'SUBPATH': '', 'ENABLE_BLOCKED_LINK_TOOLTIP': 0, 'ALLOWED_HOSTS': ['.android.com', '.api.ai', '.apigee.com', '.appspot.com', '.gonglchuangl.net', '.google.cn', '.google.com', '.googleplex.com', '.nest.com', '.openthread.io', '.orbitera.com', '.tensorflow.org'], 'BLOCK_RSS_FEEDS': 0, 'SCRIPTSAFE_DOMAIN': 'tensorflow-dot-google-developers.appspot.com'},
                               '[]','en',
                               true, '',
                               {"f62218c009ec029abef196bba5aa34cf": false, "039e5d84b87fd75807ffb37b7f1bbf2c": true, "098dafe57affddc137df300142652cfd": false, "cb025a64a50094835616312f4774a53d": true, "c95bf81627eb648c2345aec06a66200a": true, "51470233c56fc1fde50f00b73c52b216": false, "d169d485cf24243a263783dbe42029b1": false, "752953480de00a336d911a46966cc16d": false, "700def1a83e356c06c0925afb05de4b0": false, "6749dcb526ce9bde6993550c7d928d24": true}, '/',
                               'https://www.tensorflow.org/');
        })(jQuery);

        
        devsite.localInit = function() {
          
        };

      </script>
   <script>
      $('.devsite-utility-footer-language-select').each(function() {
        $(this).change(function(){$('.devsite-utility-footer-language').submit();});
      });
      </script>
  </div>
  <span id="devsite-request-elapsed" data-request-elapsed="300.429821014"></span> 
 </body>
</html>