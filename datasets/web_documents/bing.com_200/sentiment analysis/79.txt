<!doctype html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
 <!--<![endif]-->
 <head> 
  <meta charset="utf-8"> 
  <title>The Stanford Natural Language Processing Group </title> 
  <meta name="description" content=""> 
  <meta name="viewport" content="width=device-width"> 
  <!-- Bootstrap styles --> 
  <link rel="stylesheet" href="/static/css/vendor/bootstrap/bootstrap.css"> 
  <!-- Glyphicons --> 
  <link rel="stylesheet" href="/static/css/vendor/glyphicons/glyphicons.css"> 
  <link rel="stylesheet" href="/static/css/vendor/glyphicons/filetypes.css"> 
  <link rel="stylesheet" href="/static/css/vendor/glyphicons/social.css"> 
  <!-- Google Webfonts --> 
  <link href="//fonts.googleapis.com/css?family=Open+Sans:400,700,600" rel="stylesheet" type="text/css"> 
  <!-- fontawesome --> 
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"> 
  <link rel="stylesheet" href="/static/css/bootstrap-markdown.min.css" type="text/css"> 
  <!-- LayerSlider styles --> 
  <link rel="stylesheet" href="/static/css/vendor/layerslider/layerslider.css" type="text/css"> 
  <!-- Grove Styles (switch for different color schemes, e.g. "styles-cleanblue.css") --> 
  <link rel="stylesheet" href="/static/css/styles-cleanred.css" id="grove-styles"> 
  <link rel="stylesheet" href="//cdn.datatables.net/1.10.7/css/jquery.dataTables.min.css" type="text/css"> 
  <!-- table highlighting --> 
  <!--<link rel="stylesheet" href="//cdn.datatables.net/plug-ins/1.10.7/features/searchHighlight/dataTables.searchHighlight.css">--> 
  <!-- site-wide custom css --> 
  <link rel="stylesheet" href="/static/css/nlp.css" id="nlp-styles"> 
  <meta http-equiv="refresh" content="0;http://stanfordnlp.github.io/CoreNLP/"> 
  <script language="javascript">
    window.location.href = "http://stanfordnlp.github.io/CoreNLP/"
</script> 
  <!--[if lt IE 9]>
        <link rel="stylesheet" href="css/ie8.css">        
        <script src="js/vendor/google/html5-3.6-respond-1.1.0.min.js"></script>
    <![endif]--> 
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script> 
  <script>window.jQuery || document.write('<script src="static/js/vendor/jquery/jquery-1.9.1.min.js"><\/script>')</script> 
  <!-- jQuery with jQuery Easing, and jQuery Transit JS --> 
  <!-- LayerSlider from Kreatura Media with Transitions --> 
  <script src="/static/js/vendor/layerslider/greensock.js" type="text/javascript"></script> 
  <script src="/static/js/vendor/layerslider/layerslider.transitions.js" type="text/javascript"></script> 
  <script src="/static/js/vendor/layerslider/layerslider.kreaturamedia.jquery.js" type="text/javascript"></script> 
  <!-- Bootstrap Markdown for Blog editing--> 
  <script src="/static/js/markdown.js" type="text/javascript"></script> 
  <script src="/static/js/to-markdown.js" type="text/javascript"></script> 
  <script src="/static/js/bootstrap-markdown.js" type="text/javascript"></script> 
  <!-- Grove Layerslider initiation script --> 
  <script src="/static/js/grove-slider.js" type="text/javascript"></script> 
  <!-- DataTables.net for publications --> 
  <script src="//cdn.datatables.net/1.10.7/js/jquery.dataTables.min.js"></script> 
  <!-- highlighting for tables --> 
  <script src="//cdn.datatables.net/plug-ins/1.10.7/features/searchHighlight/dataTables.searchHighlight.min.js"></script> 
  <script src="//bartaz.github.io/sandbox.js/jquery.highlight.js"></script> 
  <!-- fancy select boxes --> 
  <link href="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.3/css/select2.min.css" rel="stylesheet"> 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/select2/4.0.3/js/select2.min.js"></script> 
 </head> 
 <body> 
  <!-- navigation --> 
  <header> 
   <nav class="navbar navbar-default grove-navbar"> 
    <div class="container"> 
     <div class="navbar-header"> 
      <a href="#" class="grove-toggle collapsed" data-toggle="collapse" data-target=".grove-nav"> <i class="glyphicons show_lines"></i> </a> 
      <img class="navbar-brand navbar-left hidden-xs" src="/static/img/logos/nlp-logo-small.gif" alt=""> 
      <a class="navbar-brand navbar-left" href="/"><h3 class="hidden-xs">The Stanford Natural Language Processing Group</h3><h3 class="hidden-sm hidden-md hidden-lg hidden-xl">The Stanford NLP Group</h3></a> 
     </div> 
     <div class="navbar-collapse grove-nav collapse"> 
      <ul class="nav navbar-nav"> 
       <li> <a href="/people/">people</a> </li> 
       <li> <a href="/pubs/">publications</a> </li> 
       <li> <a href="/blog/">research blog</a> </li> 
       <li> <a href="/software/">software</a> </li> 
       <li> <a href="/teaching/">teaching</a> </li> 
       <li> <a href="/new_local/">local</a> 
        <!--<a href="http://nlp.stanford.edu/local/">local</a>--> </li> 
      </ul> 
     </div>
     <!-- /.navbar-collapse --> 
    </div> 
   </nav> 
  </header> 
  <div class="widewrapper"> 
   <div id="body_content"> 
    <div class="widewrapper pagetitle"> 
     <div class="container"> 
      <h1><a href="/software/">Software</a> &gt; Stanford CoreNLP</h1> 
     </div> 
    </div> 
    <div class="widewrapper weak-highlight"> 
     <div class="container content"> 
      <div class="row"> 
       <center> 
        <h2>Stanford CoreNLP</h2> 
        <h3>A Suite of Core NLP Tools</h3> 
       </center> 
       <center>
        <p><a href="#About">About</a> | <a href="#Citing">Citing</a> | <a href="#Download">Download</a> | <a href="#Usage">Usage</a> | <a href="#SUTime">SUTime</a> | <a href="#sentiment">Sentiment</a> | <a href="#newannotators">Adding Annotators</a> | <a href="#caseless">Caseless Models</a> | <a href="#srparser">Shift Reduce Parser</a> | <a href="#Extensions">Extensions</a> | <a href="#Questions">Questions</a> | <a href="#Mail">Mailing lists</a> | <a href="http://nlp.stanford.edu:8080/corenlp/">Online demo</a> | <a href="corenlp-faq.shtml">FAQ</a> | <a href="#History">Release history</a> </p>
       </center> 
       <h3><a name="About">About</a></h3> 
       <p> Stanford CoreNLP provides a set of natural language analysis tools which can take raw text input and give the base forms of words, their parts of speech, whether they are names of companies, people, etc., normalize dates, times, and numeric quantities, and mark up the structure of sentences in terms of phrases and word dependencies, indicate which noun phrases refer to the same entities, indicate sentiment, etc. Stanford CoreNLP is an integrated framework. Its goal is to make it very easy to apply a bunch of linguistic analysis tools to a piece of text. Starting from plain text, you can run all the tools on it with just two lines of code. It is designed to be highly flexible and extensible. With a single option you can change which tools should be enabled and which should be disabled. Its analyses provide the foundational building blocks for higher-level and domain-specific text understanding applications. </p> 
       <p> Stanford CoreNLP integrates many of our NLP tools, including <a href="tagger.shtml">the part-of-speech (POS) tagger</a>, <a href="CRF-NER.shtml">the named entity recognizer (NER)</a>, <a href="lex-parser.shtml">the parser</a>, <a href="dcoref.shtml">the coreference resolution system</a>, <a href="http://nlp.stanford.edu/sentiment/">the sentiment analysis</a>, and <a href="patternslearning.shtml">the bootstrapped pattern learning</a> tools. The basic distribution provides model files for the analysis of <b>English</b>, but the engine is compatible with models for other languages. Below you can find packaged models for <b>Chinese</b> and <b>Spanish</b>, and Stanford NLP models for German and Arabic are usable inside CoreNLP. </p> 
       <p> Stanford CoreNLP is written in Java and licensed under the <a href="http://www.gnu.org/licenses/gpl.html">GNU General Public License</a> (v3 or later; in general Stanford NLP code is GPL v2+, but CoreNLP uses several Apache-licensed libraries, and so the composite is v3+). Source is included. Note that this is the <i>full</i> GPL, which allows many free uses, but not its use in <a href="http://www.gnu.org/licenses/gpl-faq.html#GPLInProprietarySystem">proprietary software</a> which is distributed to others. The download is 260 MB and requires Java 1.8+. </p> 
       <br> 
       <h3><a name="Citing">Citing Stanford CoreNLP</a></h3> 
       <p> If you're just running the CoreNLP pipeline, please cite this CoreNLP demo paper. If you're dealing in depth with particular annotators, you're also very welcome to cite the papers that cover individual components (check elsewhere on our <a href="index.shtml">software pages</a>). </p> 
       <blockquote>
         Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. 
        <a href="/pubs/StanfordCoreNlp2014.pdf">The Stanford CoreNLP Natural Language Processing Toolkit</a>. In 
        <i>Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</i>, pp. 55-60. [
        <a href="/pubs/StanfordCoreNlp2014.pdf">pdf</a>] [
        <a href="/pubs/StanfordCoreNlp2014.bib">bib</a>] 
       </blockquote> 
       <br> 
       <h3><a name="Download">Download</a></h3> 
       <center> 
        <b><font color="#a40526"><a href="stanford-corenlp-full-2015-04-20.zip"> Download Stanford CoreNLP version 3.5.2</a></font></b>. 
       </center> 
       <p> If you want to change the source code and recompile the files, see <a href="basic-compiling.txt">these instructions</a>. </p> 
       <p> <b>GitHub:</b> Here is <a href="https://github.com/stanfordnlp/CoreNLP">the Stanford CoreNLP GitHub site</a>. </p> 
       <p> <b>Maven:</b> You can find Stanford CoreNLP on <a href="http://search.maven.org/#browse%7C11864822">Maven Central</a>. The crucial thing to know is that CoreNLP needs its models to run (most parts beyond the tokenizer) and so you need to specify both the code jar and the models jar in your <code>pom.xml</code>, as follows: </p> 
       <p>(Note: Maven releases are made several days after the release on the website.)</p> 
       <blockquote>
        <pre>
&lt;dependencies&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.5.2&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.5.2&lt;/version&gt;
    &lt;classifier&gt;models&lt;/classifier&gt;
&lt;/dependency&gt;
&lt;/dependencies&gt;
</pre>
       </blockquote> 
       <p>NEW: If you want to get a language models jar off of Maven for Chinese, Spanish, or German, add this to your <code>pom.xml</code>:</p> 
       <blockquote>
        <pre>
&lt;dependency&gt;
    &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt;
    &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt;
    &lt;version&gt;3.5.2&lt;/version&gt;
    &lt;classifier&gt;models-chinese&lt;/classifier&gt;
&lt;/dependency&gt;
</pre>
       </blockquote> 
       <p>Replace "models-chinese" with "models-german" or "models-spanish" for the other two languages!</p> 
       <br> 
       <h3><a name="Usage">Usage</a></h3> 
       <p><b><u>Parsing a file and saving the output as XML</u></b></p> 
       <p> Before using Stanford CoreNLP, it is usual to create a configuration file (a Java Properties file). Minimally, this file should contain the "annotators" property, which contains a comma-separated list of Annotators to use. For example, the setting below enables: tokenization, sentence splitting (required by most Annotators), POS tagging, lemmatization, NER, syntactic parsing, and coreference resolution. </p> 
       <pre>
annotators = tokenize, ssplit, pos, lemma, ner, parse, dcoref
</pre> 
       <p> However, if you just want to specify one or two properties, you can instead place them on the command line. </p> 
       <p> To process one file using Stanford CoreNLP, use the following sort of command line (adjust the JAR file date extensions to your downloaded release): </p> 
       <pre>
java -cp stanford-corenlp-VV.jar:stanford-corenlp-VV-models.jar:xom.jar:joda-time.jar:jollyday.jar:ejml-VV.jar -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP [ -props &lt;YOUR CONFIGURATION FILE&gt; ] -file &lt;YOUR INPUT FILE&gt;
</pre> In particular, to process the included sample file 
       <code>input.txt</code> you can use this command in the distribution directory (where we use a wildcard after 
       <tt>-cp</tt> to load all jar files in the directory): 
       <pre>
java -cp "*" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt
</pre> Notes: 
       <ul> 
        <li>Stanford CoreNLP requires Java version 1.8 or higher. </li>
        <li><b>Specifying memory:</b> <code>-Xmx2g</code> specifies the amount of RAM that Java will make available for CoreNLP. On a 64-bit machine, Stanford CoreNLP typically requires 2GB to run (and it may need even more, depending on the size of the document to parse). On a 32 bit machine, you cannot allocate 2GB of RAM, probably you should try with 
         <coe>
          -Xmx1800m, but this amount of memory is a bit marginal. This is especially a problem on 32-bit Windows machines.
         </coe></li> 
        <li>The first command above works for Mac OS X or Linux. For Windows, the colons (:) separating the jar files need to be semi-colons (;). And, if you are not sitting in the distribution directory, you'll also need to include a path to the files before each. </li>
        <li>The -annotators argument is actually optional. If you leave it out, the code uses a built in properties file, which enables the following annotators: tokenization and sentence splitting, POS tagging, lemmatization, NER, parsing, and coreference resolution (that is, what we used in this example).</li> 
        <li>Processing a short text like this is very inefficient. It takes a minute to load everything before processing begins. You should batch your processing.</li> 
       </ul> 
       <p> Stanford CoreNLP includes an interactive shell for analyzing sentences. If you do not specify any properties that load input files, you will be placed in the interactive shell. Type <tt>q</tt> to exit: </p> 
       <pre>
java -cp "*" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref
</pre> 
       <p> If you want to process a list of files use the following command line: </p> 
       <pre>
java -cp stanford-corenlp-VV.jar:stanford-corenlp-VV-models.jar:xom.jar:joda-time.jar:jollyday.jar:ejml-VV.jar -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP [ -props &lt;YOUR CONFIGURATION FILE&gt; ] -filelist &lt;A FILE CONTAINING YOUR LIST OF FILES&gt;
</pre> 
       <p> where the <code>-filelist</code> parameter points to a file whose content lists all files to be processed (one per line). </p> 
       <p> Note that the <code>-props</code> parameter is optional. By default, Stanford CoreNLP will search for <code>StanfordCoreNLP.properties</code> in your classpath and use the defaults included in the distribution. </p> 
       <p> By default, output files are written to the current directory. You may specify an alternate output directory with the flag <code>-outputDirectory</code>. Output filenames are the same as input filenames but with <code>-outputExtension</code> added them (<code>.xml</code> by default). It will overwrite (clobber) output files by default. Pass <code> -noClobber</code> to avoid this behavior. Additionally, if you'd rather it replace the extension with the <code>-outputExtension</code>, pass the <code>-replaceExtension</code> flag. This will result in filenames like <code>test.xml</code> instead of <code>test.txt.xml</code> (when given <code>test.txt</code> as an input file). </p> 
       <p> For each input file, Stanford CoreNLP generates one file (an XML or text file) with all relevant annotation. For example, for the above configuration and a file containing the text below: </p> 
       <pre>
Stanford University is located in California. It is a great university.
</pre> 
       <p> Stanford CoreNLP generates the <a href="corenlp_output.html">following output</a>, with the <a href="corenlp_xml_description.shtml">following attributes</a>. </p> 
       <p> Note that the XML output uses the CoreNLP-to-HTML.xsl stylesheet file, which can be downloaded from <a href="CoreNLP-to-HTML.xsl">here</a>. This stylesheet enables human-readable display of the above XML content. For example, the previous example should be displayed like <a href="example.xml">this</a>. </p> 
       <p> Stanford CoreNLP also has the ability to remove most XML from a document before processing it. (CDATA is not correctly handled.) For example, if run with the annotators </p> 
       <pre>
annotators = tokenize, cleanxml, ssplit, pos, lemma, ner, parse, dcoref
</pre> 
       <p> and given the text </p> 
       <pre>
&lt;xml&gt;Stanford University is located in California. It is a great university.&lt;/xml&gt;
</pre> Stanford CoreNLP generates the 
       <a href="corenlp_output2.html">following output</a>. Note that the only difference between this and 
       <a href="corenlp_output.html">the original output</a> is the change in CharacterOffsets. 
       <br> 
       <p><b><u><a name="api">Using the Stanford CoreNLP API</a></u></b></p> 
       <p> The backbone of the CoreNLP package is formed by two classes: Annotation and Annotator. Annotations are the data structure which hold the results of annotators. Annotations are basically maps, from keys to bits of the annotation, such as the parse, the part-of-speech tags, or named entity tags. Annotators are a lot like functions, except that they operate over Annotations instead of Objects. They do things like tokenize, parse, or NER tag sentences. Annotators and Annotations are integrated by AnnotationPipelines, which create sequences of generic Annotators. Stanford CoreNLP inherits from the AnnotationPipeline class, and is customized with NLP Annotators. </p> 
       <p> The table below summarizes the Annotators currently supported and the Annotations that they generate. </p>
       <p> </p>
       <table width="90%" align="center" style="border: 1px dashed; border-color:#777777" cellspacing="15"> 
        <tbody>
         <tr>
          <td align="center" width="10%"><b>Property name</b></td>
          <td align="center" width="20%"><b>Annotator class name</b></td>
          <td align="center" width="20%"><b>Generated Annotation</b></td>
          <td align="left" width="50%"><b>Description</b></td>
         </tr> 
         <tr>
          <td align="center">tokenize</td>
          <td align="center">TokenizerAnnotator</td>
          <td align="center">TokensAnnotation (list of tokens), and CharacterOffsetBeginAnnotation, CharacterOffsetEndAnnotation, TextAnnotation (for each token) </td>
          <td> Tokenizes the text. This component started as a PTB-style tokenizer, but was extended since then to handle noisy and web text. The tokenizer saves the character offsets of each token in the input text, as CharacterOffsetBeginAnnotation and CharacterOffsetEndAnnotation. </td>
         </tr> 
         <tr>
          <td align="center">cleanxml</td>
          <td align="center">CleanXmlAnnotator</td>
          <td align="center">XmlContextAnnotation</td>
          <td>Remove xml tokens from the document</td>
         </tr> 
         <tr>
          <td align="center">ssplit</td>
          <td align="center">WordToSentenceAnnotator</td>
          <td align="center">SentencesAnnotation</td>
          <td>Splits a sequence of tokens into sentences.</td>
         </tr> 
         <tr>
          <td align="center">pos</td>
          <td align="center">POSTaggerAnnotator</td>
          <td align="center">PartOfSpeechAnnotation</td>
          <td>Labels tokens with their POS tag. For more details see <a href="tagger.shtml">this page</a>.</td>
         </tr> 
         <tr>
          <td align="center">lemma</td>
          <td align="center">MorphaAnnotator</td>
          <td align="center">LemmaAnnotation</td>
          <td>Generates the word lemmas for all tokens in the corpus.</td>
         </tr> 
         <tr>
          <td align="center">ner</td>
          <td align="center">NERClassifierCombiner</td>
          <td align="center">NamedEntityTagAnnotation and NormalizedNamedEntityTagAnnotation</td>
          <td>Recognizes named (PERSON, LOCATION, ORGANIZATION, MISC), numerical (MONEY, NUMBER, ORDINAL, PERCENT), and temporal (DATE, TIME, DURATION, SET) entities. Named entities are recognized using a combination of three CRF sequence taggers trained on various corpora, such as ACE and MUC. Numerical entities are recognized using a rule-based system. Numerical entities that require normalization, e.g., dates, are normalized to NormalizedNamedEntityTagAnnotation. For more details on the CRF tagger see <a href="CRF-NER.shtml">this page</a>.</td>
         </tr> 
         <tr>
          <td align="center">regexner</td>
          <td align="center">RegexNERAnnotator</td>
          <td align="center">NamedEntityTagAnnotation</td>
          <td>Implements a simple, rule-based NER over token sequences using Java regular expressions. The goal of this Annotator is to provide a simple framework to incorporate NE labels that are not annotated in traditional NL corpora. For example, the default list of regular expressions that we distribute in the models file recognizes ideologies (IDEOLOGY), nationalities (NATIONALITY), religions (RELIGION), and titles (TITLE). Here is <a href="regexner/"> a simple example</a> of how to use RegexNER. For more complex applications, you might consider <a href="http://nlp.stanford.edu/software/tokensregex.shtml">TokensRegex</a>.</td>
         </tr> 
         <tr> 
          <td align="center">sentiment</td> 
          <td align="center">SentimentAnnotator</td> 
          <td align="center">SentimentCoreAnnotations.AnnotatedTree</td> 
          <td>Implements Socher et al's sentiment model. Attaches a binarized tree of the sentence to the sentence level CoreMap. The nodes of the tree then contain the annotations from RNNCoreAnnotations indicating the predicted class and scores for that subtree. See the <a href="http://nlp.stanford.edu/sentiment/">sentiment page</a> for more information about this project. </td> 
         </tr> 
         <tr>
          <td align="center">truecase</td>
          <td align="center">TrueCaseAnnotator</td>
          <td align="center">TrueCaseAnnotation and TrueCaseTextAnnotation</td>
          <td>Recognizes the true case of tokens in text where this information was lost, e.g., all upper case text. This is implemented with a discriminative model implemented using a CRF sequence tagger. The true case label, e.g., INIT_UPPER is saved in TrueCaseAnnotation. The token text adjusted to match its true case is saved as TrueCaseTextAnnotation.</td>
         </tr> 
         <tr>
          <td align="center">parse</td>
          <td align="center">ParserAnnotator</td>
          <td align="center">TreeAnnotation, BasicDependenciesAnnotation, CollapsedDependenciesAnnotation, CollapsedCCProcessedDependenciesAnnotation</td>
          <td>Provides full syntactic analysis, using both the constituent and the dependency representations. The constituent-based output is saved in TreeAnnotation. We generate three dependency-based outputs, as follows: basic, uncollapsed dependencies, saved in BasicDependenciesAnnotation; collapsed dependencies saved in CollapsedDependenciesAnnotation; and collapsed dependencies with processed coordinations, in CollapsedCCProcessedDependenciesAnnotation. Most users of our parser will prefer the latter representation. For more details on the parser, please see <a href="lex-parser.shtml">this page</a>. For more details about the dependencies, please refer to <a href="stanford-dependencies.shtml">this page</a>. </td>
         </tr> 
         <tr> 
          <td align="center">depparse</td> 
          <td align="center">DependencyParseAnnotator</td> 
          <td align="center">BasicDependenciesAnnotation, CollapsedDependenciesAnnotation, CollapsedCCProcessedDependenciesAnnotation</td> 
          <td>Provides a fast syntactic dependency parser. We generate three dependency-based outputs, as follows: basic, uncollapsed dependencies, saved in BasicDependenciesAnnotation; collapsed dependencies saved in CollapsedDependenciesAnnotation; and collapsed dependencies with processed coordinations, in CollapsedCCProcessedDependenciesAnnotation. Most users of our parser will prefer the latter representation. For details about the dependency software, see <a href="nndep.shtml">this page</a>. For more details about dependency parsing in general, see <a href="stanford-dependencies.shtml">this page</a>.</td> 
         </tr> 
         <tr>
          <td align="center">dcoref</td>
          <td align="center">DeterministicCorefAnnotator</td>
          <td align="center">CorefChainAnnotation</td>
          <td>Implements both pronominal and nominal coreference resolution. The entire coreference graph (with head words of mentions as nodes) is saved in CorefChainAnnotation. For more details on the underlying coreference resolution algorithm, see <a href="dcoref.shtml">this page</a>.</td>
         </tr> 
         <tr> 
          <td align="center">relation</td> 
          <td align="center">RelationExtractorAnnotator</td> 
          <td align="center">MachineReadingAnnotations.RelationMentionsAnnotation</td> 
          <td>Stanford relation extractor is a Java implementation to find relations between two entities. The current relation extraction model is trained on the relation types (except the 'kill' relation) and data from the paper Roth and Yih, Global inference for entity and relation identification via a linear programming formulation, 2007, except instead of using the gold NER tags, we used the NER tags predicted by Stanford NER classifier to improve generalization. The default model predicts relations <tt>Live_In</tt>, <tt>Located_In</tt>, <tt>OrgBased_In</tt>, <tt>Work_For</tt>, and <tt>None</tt>. For more details of how to use and train your own model, see <a href="relationExtractor.shtml">this page</a>. </td> 
         </tr> 
         <tr> 
          <td align="center">natlog</td> 
          <td align="center">NaturalLogicAnnotator</td> 
          <td align="center">OperatorAnnotation, PolarityAnnotation</td> 
          <td>Marks quantifier scope and token polarity, according to natural logic semantics. Places an OperatorAnnotation on tokens which are quantifiers (or other natural logic operators), and a PolarityAnnotation on all tokens in the sentence.</td> 
         </tr> 
         <tr> 
          <td align="center">quote</td> 
          <td align="center">QuoteAnnotator</td> 
          <td align="center">QuotationAnnotation</td> 
          <td>Deterministically picks out quotes delimited by “ or ‘ from a text. All top-level quotes, are supplied by the top level annotation for a text. If a QuotationAnnotation corresponds to a quote that contains embedded quotes, these quotes will appear as embedded QuotationAnnotations that can be accessed from the QuotationAnnotation that they are embedded in. The QuoteAnnotator can handle multi-line and cross-paragraph quotes, but any embedded quotes must be delimited by a different kind of quotation mark than its parents. Does not depend on any other annotators. Support for unicode quotes is not yet present.</td> 
         </tr> 
         <tr> 
          <td align="center">entitymentions</td> 
          <td align="center">EntityMentionsAnnotator</td> 
          <td align="center">MentionsAnnotation</td> 
          <td>Provides a list of the mentions identified by NER (including their spans, NER tag, normalized value, and time). As an instance, "New York City" will be identified as one mention spanning three tokens.</td> 
         </tr> 
        </tbody>
       </table> 
       <p> Depending on which annotators you use, please cite the corresponding papers on: <a href="tagger.shtml">POS tagging</a>, <a href="CRF-NER.shtml">NER</a>, <a href="lex-parser.shtml">parsing</a> (with <tt>parse</tt> annotator), <a href="nndep.shtml">dependency parsing</a> (with <tt>depparse</tt> annotator), <a href="dcoref.shtml">coreference resolution</a>, or <a href="http://nlp.stanford.edu/sentiment/">sentiment</a>. </p>
       <p> To construct a Stanford CoreNLP object from a given set of properties, use <code>StanfordCoreNLP(Properties props)</code>. This method creates the pipeline using the annotators given in the "annotators" property (see above for an example setting). The complete list of accepted annotator names is listed in the first column of the table above. To parse an arbitrary text, use the <code>annotate(Annotation document)</code> method. </p>
       <p> The code below shows how to create and use a Stanford CoreNLP object: </p>
       <p> </p>
       <pre>
    // creates a StanfordCoreNLP object, with POS tagging, lemmatization, NER, parsing, and coreference resolution 
    Properties props = new Properties();
    props.setProperty("annotators", "tokenize, ssplit, pos, lemma, ner, parse, dcoref");
    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
    
    // read some text in the text variable
    String text = ... // Add your text here!
    
    // create an empty Annotation just with the given text
    Annotation document = new Annotation(text);
    
    // run all Annotators on this text
    pipeline.annotate(document);
    
    // these are all the sentences in this document
    // a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
    List&lt;CoreMap&gt; sentences = document.get(SentencesAnnotation.class);
    
    for(CoreMap sentence: sentences) {
      // traversing the words in the current sentence
      // a CoreLabel is a CoreMap with additional token-specific methods
      for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
        // this is the text of the token
        String word = token.get(TextAnnotation.class);
        // this is the POS tag of the token
        String pos = token.get(PartOfSpeechAnnotation.class);
        // this is the NER label of the token
        String ne = token.get(NamedEntityTagAnnotation.class);       
      }

      // this is the parse tree of the current sentence
      Tree tree = sentence.get(TreeAnnotation.class);

      // this is the Stanford dependency graph of the current sentence
      SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
    }

    // This is the coreference link graph
    // Each chain stores a set of mentions that link to each other,
    // along with a method for getting the most representative mention
    // Both sentence and token offsets start at 1!
    Map&lt;Integer, CorefChain&gt; graph = 
      document.get(CorefChainAnnotation.class);
</pre> 
       <p><b><u>Annotator options</u></b> </p>
       <p> While all Annotators have a default behavior that is likely to be sufficient for the majority of users, most Annotators take additional options that can be passed as Java properties in the configuration file. We list below the configuration options for all Annotators: </p>
       <p> <b>general options</b>:<br> </p>
       <ul> 
        <li>outputFormat: different methods for outputting results. Can be "xml", "text" or "serialized". </li>
        <li>annotators: which annotators to use. </li>
        <li>encoding: the character encoding or charset. The default is "UTF-8". </li>
       </ul> 
       <b>tokenize</b>:
       <br> 
       <ul> 
        <li>tokenize.whitespace: if set to true, separates words only when whitespace is encountered. </li>
        <li>tokenize.options: Accepts the options of <code>PTBTokenizer</code> for example, things like "americanize=false" or "strictTreebank3=true,untokenizable=allKeep". </li>
       </ul> 
       <b>cleanxml</b>:
       <br> 
       <ul> 
        <li>clean.xmltags: Discard xml tag tokens that match this regular expression. For example, .* will discard all xml tags. </li>
        <li>clean.sentenceendingtags: treat tags that match this regular expression as the end of a sentence. For example, p will treat &lt;p&gt; as the end of a sentence. </li>
        <li>clean.allowflawedxml: if this is true, allow errors such as unclosed tags. Otherwise, such xml will cause an exception. </li>
        <li>clean.datetags: a regular expression that specifies which tags to treat as the reference date of a document. Defaults to datetime|date </li>
       </ul> 
       <b>ssplit</b>:
       <br> 
       <ul> 
        <li>ssplit.eolonly: only split sentences on newlines. Works well in conjunction with "-tokenize.whitespace true", in which case StanfordCoreNLP will treat the input as one sentence per line, only separating words on whitespace. </li>
        <li>ssplit.isOneSentence: each document is to be treated as one sentence, no sentence splitting at all. </li>
        <li>ssplit.newlineIsSentenceBreak: Whether to treat newlines as sentence breaks. This property has 3 legal values: "always", "never", or "two". The default is "never". "always" means that a newline is always a sentence break (but there still may be multiple sentences per line). This is often appropriate for texts with soft line breaks. "never" means to ignore newlines for the purpose of sentence splitting. This is appropriate when just the non-whitespace characters should be used to determine sentence breaks. "two" means that two or more consecutive newlines will be treated as a sentence break. This option can be appropriate when dealing with text with hard line breaking, and a blank line between paragraphs. 
         <ul> 
          <li>A side-effect of setting ssplit.newlineIsSentenceBreak to "two" or "always" is that tokenizer will tokenize newlines.</li> 
         </ul> </li>
        <li>ssplit.boundaryMultiTokenRegex: Value is a multi-token sentence boundary regex. </li>
        <li>ssplit.boundaryTokenRegex: </li>
        <li>ssplit.boundariesToDiscard: </li>
        <li>ssplit.htmlBoundariesToDiscard </li>
        <li>ssplit.tokenPatternsToDiscard: </li>
       </ul> 
       <b>pos</b>:
       <br> 
       <ul> 
        <li>pos.model: POS model to use. There is no need to explicitly set this option, unless you want to use a different POS model (for advanced developers only). By default, this is set to the english left3words POS model included in the stanford-corenlp-models JAR file. </li>
        <li>pos.maxlen: Maximum sentence size for the POS sequence tagger. Useful to control the speed of the tagger on noisy text without punctuation marks. Note that the parser, if used, will be much more expensive than the tagger. </li>
       </ul> 
       <b>ner</b>:
       <br> 
       <ul> 
        <li>ner.useSUTime: Whether or not to use sutime. On by default in the version which includes sutime, off by default in the version that doesn't. If not processing English, make sure to set this to false. </li>
        <li>ner.model: NER model(s) in a comma separated list to use instead of the default models. By default, the models used will be the 3class, 7class, and MISCclass models, in that order. </li>
        <li>ner.applyNumericClassifiers: Whether or not to use numeric classifiers, including <a href="sutime.shtml">SUTime</a>. These are hardcoded for English, so if using a different language, this should be set to false. </li>
        <li>sutime.markTimeRanges: Tells sutime to mark phrases such as "From January to March" instead of marking "January" and "March" separately </li>
        <li>sutime.includeRange: If marking time ranges, set the time range in the TIMEX output from sutime </li>
       </ul> 
       <b>regexner</b>:
       <br> 
       <ul> 
        <li>regexner.mapping: The name of a file, classpath, or URI that contains NER rules, i.e., the mapping from regular expressions to NE classes. The format is one rule per line; each rule has two mandatory fields separated by one tab. The first field stores one or more Java regular expression (without any slashes or anything around them) separated by non-tab whitespace. The second token gives the named entity class to assign when the regular expression matches one or a sequence of tokens. An optional third tab-separated field indicates which regular named entity types can be overwritten by the current rule. For example, the rule "U\.S\.A\. COUNTRY LOCATION" marks the token "U.S.A." as a COUNTRY, allowing overwriting the previous LOCATION label (if it exists). An optional fourth tab-separated field gives a real number-valued rule priority. Higher priority rules are tried first for matches. Here is <a href="regexner/"> a simple example</a>. </li>
        <li>regexner.ignorecase: if set to true, matching will be case insensitive. Default value is false. In the simplest case, the mapping file can be just a word list of lines of "word TAB class". Especially in this case, it may be easiest to set this to true, so it works regardless of capitalization. </li>
        <li>regexner.validpospattern: If given (non-empty and non-null) this is a regex that must be matched (with <code>find()</code>) againstat least one token in a match for the NE to be labeled. </li>
       </ul> 
       <b>parse</b>:
       <br> 
       <ul> 
        <li>parse.model: parsing model to use. There is no need to explicitly set this option, unless you want to use a different parsing model (for advanced developers only). By default, this is set to the parsing model included in the stanford-corenlp-models JAR file. </li>
        <li>parse.maxlen: if set, the annotator parses only sentences shorter (in terms of number of tokens) than this number. For longer sentences, the parser creates a flat structure, where every token is assigned to the non-terminal X. This is useful when parsing noisy web text, which may generate arbitrarily long sentences. By default, this option is not set. </li>
        <li>parse.flags: flags to use when loading the parser model. The English model used by default uses "-retainTmpSubcategories" </li>
        <li>parse.originalDependencies: Generate original Stanford Dependencies grammatical relations instead of Universal Dependencies. Note, however, that some annotators that use dependencies such as natlog might not function properly if you use this option. <br> If you are using the <a href="nndep.shtml">Neural Network dependency parser</a> and want to get the original SD relations, see the <a href="corenlp-faq.shtml#originaldeps">CoreNLP FAQ</a> on how to use a model trained on Stanford Dependencies.</li> 
       </ul> 
       <b>depparse</b>:
       <br> 
       <ul> 
        <li>depparse.model: dependency parsing model to use. There is no need to explicitly set this option, unless you want to use a different parsing model than the default. By default, this is set to the UD parsing model included in the stanford-corenlp-models JAR file. </li> 
        <li>depparse.extradependencies: Whether to include extra (enhanced) dependencies in the output. The default is NONE (basic dependencies) and this can have other values of the GrammaticalStructure.Extras enum, such as SUBJ_ONLY or MAXIMAL (all extra dependencies). </li> 
       </ul> 
       <b>dcoref</b>:
       <br> 
       <ul> 
        <li>dcoref.sievePasses: list of sieve modules to enable in the system, specified as a comma-separated list of class names. By default, this property is set to include: "edu.stanford.nlp.dcoref.sievepasses.MarkRole, edu.stanford.nlp.dcoref.sievepasses.DiscourseMatch, edu.stanford.nlp.dcoref.sievepasses.ExactStringMatch, edu.stanford.nlp.dcoref.sievepasses.RelaxedExactStringMatch, edu.stanford.nlp.dcoref.sievepasses.PreciseConstructs, edu.stanford.nlp.dcoref.sievepasses.StrictHeadMatch1, edu.stanford.nlp.dcoref.sievepasses.StrictHeadMatch2, edu.stanford.nlp.dcoref.sievepasses.StrictHeadMatch3, edu.stanford.nlp.dcoref.sievepasses.StrictHeadMatch4, edu.stanford.nlp.dcoref.sievepasses.RelaxedHeadMatch, edu.stanford.nlp.dcoref.sievepasses.PronounMatch". The default value can be found in Constants.SIEVEPASSES. </li>
        <li>dcoref.demonym: list of demonyms from <a href="http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names">http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names</a>. The format of this file is: location TAB singular gentilic form TAB plural gentilic form, e.g., "Algeria Algerian Algerians". </li>
        <li>dcoref.animate and dcoref.inanimate: lists of animate/inanimate words, from (Ji and Lin, 2009). The format is one word per line. </li>
        <li>dcoref.male, dcoref.female, dcoref.neutral: lists of words of male/female/neutral gender, from (Bergsma and Lin, 2006) and (Ji and Lin, 2009). The format is one word per line. </li>
        <li>dcoref.plural and dcoref.singular: lists of words that are plural or singular, from (Bergsma and Lin, 2006). The format is one word per line. All the above dictionaries are already set to the files included in the stanford-corenlp-models JAR file, but they can easily be adjusted to your needs by setting these properties. </li>
        <li>dcoref.maxdist: the maximum distance at which to look for mentions. Can help keep the runtime down in long documents. </li>
        <li>oldCorefFormat: produce a CorefGraphAnnotation, the output format used in releases v1.0.3 or earlier. Note that this uses quadratic memory rather than linear. </li>
       </ul> 
       <b>sentiment</b>:
       <br> 
       <ul> 
        <li>sentiment.model: which model to load. Will default to the model included in the models jar. </li>
       </ul> 
       <b>quote</b>:
       <br> 
       <ul> 
        <li>quote.singleQuotes: whether or not to consider single quotes as quote delimiters. Default is "false". </li>
       </ul> 
       <p><b><u>Javadoc</u></b> </p>
       <p> More information is available in the javadoc: <a href="http://nlp.stanford.edu/nlp/javadoc/javanlp/"> Stanford Core NLP Javadoc</a>. </p>
       <h3><a name="SUTime">SUTime</a></h3> 
       <p> StanfordCoreNLP includes <a href="sutime.shtml">SUTime</a>, Stanford's temporal expression recognizer. SUTime is transparently called from the "ner" annotator, so no configuration is necessary. Furthermore, the "cleanxml" annotator now extracts the reference date for a given XML document, so relative dates, e.g., "yesterday", are transparently normalized with no configuration necessary. </p>
       <p> SUTime supports the same annotations as before, i.e., NamedEntityTagAnnotation is set with the label of the numeric entity (DATE, TIME, DURATION, MONEY, PERCENT, or NUMBER) and NormalizedNamedEntityTagAnnotation is set to the value of the normalized temporal expression. Note that NormalizedNamedEntityTagAnnotation now follows the TIMEX3 standard, rather than Stanford's internal representation, e.g., "2010-01-01" for the string "January 1, 2010", rather than "20100101". </p>
       <p> Also, SUTime now sets the TimexAnnotation key to an edu.stanford.nlp.time.Timex object, which contains the complete list of TIMEX3 fields for the corresponding expressions, such as "val", "alt_val", "type", "tid". This might be useful to developers interested in recovering complete TIMEX3 expressions. </p>
       <p> Reference dates are by default extracted from the "datetime" and "date" tags in an xml document. To set a different set of tags to use, use the clean.datetags property. When using the API, reference dates can be added to an <code>Annotation</code> via <code>edu.stanford.nlp.ling.CoreAnnotations.DocDateAnnotation</code>, although note that when processing an xml document, the cleanxml annotator will overwrite the <code>DocDateAnnotation</code> if "datetime" or "date" are specified in the document. </p> 
       <h3><a name="sentiment">Sentiment</a></h3> 
       <p> StanfordCoreNLP also includes the sentiment tool and various programs which support it. The model can be used to analyze text as part of StanfordCoreNLP by adding "sentiment" to the list of annotators. There is also command line support and model training support. For more information, please see the description on <a href="http://nlp.stanford.edu/sentiment/code.html">the sentiment project home page</a>. </p> 
       <h3><a name="tokensregex">TokensRegex</a></h3> 
       <p> StanfordCoreNLP includes <a href="tokensregex.shtml">TokensRegex</a>, a framework for defining regular expressions over text and tokens, and mapping matched text to semantic objects. </p> 
       <h3><a name="bootstrapped">Bootstrapped Surface Patterns</a></h3> 
       <p> StanfordCoreNLP includes <a href="patternslearning.shtml">Bootstrapped Pattern Learning</a>, a framework for learning patterns to learn entities of given entity types from unlabeled text starting with seed sets of entities. </p> 
       <h3><a name="newannotators">Adding a new annotator</a></h3> 
       <p> StanfordCoreNLP also has the capacity to add a new annotator by reflection without altering the code in StanfordCoreNLP.java. To create a new annotator, extend the class edu.stanford.nlp.pipeline.Annotator and define a constructor with the signature (String, Properties). Then, add the property customAnnotatorClass.FOO=BAR to the properties used to create the pipeline. If FOO is then added to the list of annotators, the class BAR will be created, with the name used to create it and the properties file passed in. </p> 
       <h3><a name="caseless">Caseless Models</a></h3> 
       <p> It is possible to run StanfordCoreNLP with tagger, parser, and NER models that ignore capitalization. In order to do this, download the <a href="stanford-corenlp-caseless-2015-04-20-models.jar">caseless models</a> package. Be sure to include the path to the case insensitive models jar in the <code>-cp</code> classpath flag as well. Then, set properties which point to these models as follows: <br> <code> -pos.model edu/stanford/nlp/models/pos-tagger/english-caseless-left3words-distsim.tagger <br> -parse.model edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz <br> -ner.model edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz <br> &nbsp;&nbsp; edu/stanford/nlp/models/ner/english.muc.7class.caseless.distsim.crf.ser.gz <br> &nbsp;&nbsp; edu/stanford/nlp/models/ner/english.conll.4class.caseless.distsim.crf.ser.gz <br> </code> </p> 
       <h3><a name="srparser">Shift Reduce Parser</a></h3> 
       <p> There is a much faster and more memory efficient parser available in the shift reduce parser. It takes quite a while to load, and the download is much larger, which is the main reason it is not the default. </p>
       <p> Details on how to use it are available on the <a href="srparser.shtml">shift reduce parser</a> page. </p> 
       <h3><a name="Extensions">Extensions: Packages and models by others using Stanford CoreNLP</a></h3> 
       <h4>Annotators/models</h4> 
       <ul> 
        <li><a href="https://github.com/jconwell/coreNlp">A stopword removal annotator</a> by John Conwell. </li> 
        <li>English informal text: <a href="https://gate.ac.uk/wiki/twitter-postagger.html">aGATE Twitter part-of-speech tagger</a>, including <a href="http://downloads.gate.ac.uk/twitie/gate-EN-twitter.model">a <code>pos.model</code> loadable by CoreNLP. </a></li>
        <a href="http://downloads.gate.ac.uk/twitie/gate-EN-twitter.model"> </a>
        <li><a href="http://downloads.gate.ac.uk/twitie/gate-EN-twitter.model">Swedish: </a><a href="https://github.com/klintan/corenlp-swedish-pos-model">Swedish POS tagger model</a> by Andreas Klintberg. See <a href="https://medium.com/@klintcho/training-a-swedish-pos-tagger-for-stanford-corenlp-546e954a8ee7">medium post</a> for details. </li>
       </ul> 
       <p>We're happy to list other models and annotators that work with Stanford CoreNLP. If you have something, please get in touch! </p> 
       <h4>Java</h4> 
       <ul> 
        <li><a href="https://code.google.com/p/dkpro-core-gpl/">dkpro-core-gpl</a> is a collection of GPL NLP components, principally Stanford CoreNLP, wrapped as <b>UIMA</b> components, based on work at the Ubiquitous Knowledge Processing Lab (UKP) at the Technische Universität Darmstadt. It is part of the <a href="http://www.ukp.tu-darmstadt.de/research/current-projects/dkpro/">DKPro</a> project. See also the <a href="http://code.google.com/p/dkpro-core-asl/wiki/WikiEntryPage">DKPro Core wiki</a> and <a href="https://code.google.com/p/dkpro-core-asl/wiki/StanfordCoreComponents">a tutorial on the Stanford CoreNLP components</a>. It is up-to-date, well-maintained, and our recommended way of using Stanford CoreNLP within UIMA. </li>
        <li><a href="http://cleartk.googlecode.com/git/cleartk-stanford-corenlp/">cleartk-stanford-corenlp</a> is a <b>UIMA</b> wrapper for Stanford CoreNLP built by Steven Bethard in the context of the <a href="http://code.google.com/p/cleartk/">ClearTK</a> toolkit. </li>
        <li><a href="https://github.com/jonnywray/mod-stanford-corenlp">A <b>Vert.x</b> module for acccessing Stanford CoreNLP</a> by Jonny Wray. </li>
        <li><a href="https://github.com/guokr/stan-cn-nlp"> Wrapper for each of Stanford's Chinese tools</a> by Mingli Yuan. </li>
        <li><a href="https://github.com/westei/stanbol-stanfordnlp">RESTful API for integrating between Stanford CoreNLP and </a><a href="https://stanbol.apache.org">Apache Stanbol</a> by Rupert Westenthaler and Cristian Petroaca. </li> 
       </ul> 
       <h4>Thrift server</h4> 
       <ul> 
        <li><a href="https://github.com/EducationalTestingService/stanford-thrift">Apache Thrift server for Stanford CoreNLP</a> by Diane Napolitano. (Written in Java, but usable from many languages.) </li>
       </ul> 
       <h4>C#/F#/.NET</h4> 
       <ul> 
        <li><a href="http://sergey-tihon.github.io/Stanford.NLP.NET/StanfordCoreNLP.html">Stanford CoreNLP for .NET</a> by Sergey Tihon. (See also: <a href="https://www.nuget.org/packages/Stanford.NLP.CoreNLP/">NuGet page</a>.) </li>
       </ul> 
       <h4>Python</h4> 
       <ul> 
        <li><a href="https://github.com/brendano/stanford-corepywrapper">Brendan O'Connor's Python wrapper</a> or maybe <a href="https://github.com/johnb30/stanford-corepywrapper">John Beieler's fork</a>. At CoreNLP v3.5.0, last we checked. </li>
        <li><a href="https://bitbucket.org/torotoki/corenlp-python">An up-to-date fork of Smith (below) by Hiroyoshi Komatsu and Johannes Castner</a> (see also: <a href="https://pypi.python.org/pypi/corenlp-python">PyPI page</a>). At CoreNLP v3.4.1, last we checked. </li>
        <li><a href="https://github.com/Wordseer/stanford-corenlp-python">A Python wrapper for Stanford CoreNLP</a> (see also: <a href="https://pypi.python.org/pypi/stanford-corenlp-python">PyPI page</a>). This "Wordseer fork" seems to merge the work of a number of people building on the original Dustin Smith wrapper, namely: Hiroyoshi Komatsu, Johannes Castner, Robert Elwell, Tristan Chong, Aditi Muralidharan. At Stanford CoreNLP v3.2.0, last we checked. See also <a href="https://github.com/relwell/stanford-corenlp-python">Robert Elwell's version</a> (also at CoreNLP v3.2.0, last we checked). </li>
        <li><a href="https://github.com/kedz/corenlp">A Python wrapper for Stanford CoreNLP</a> by Chris Kedzie (see also: <a href="https://pypi.python.org/pypi/corenlp">PyPI page</a>). At Stanford CoreNLP v3.2.0, last we checked. </li>
        <li><a href="https://github.com/dasmith/stanford-corenlp-python">Original Python wrapper including JSON-RPC server</a> by Dustin Smith. At CoreNLP v1.3.3, last we checked. </li>
       </ul> 
       <h4>Ruby</h4> 
       <ul> 
        <li><a href="https://github.com/louismullie/stanford-core-nlp"> Ruby bindings</a> by Louis Mullie (see also: <a href="https://rubygems.org/gems/stanford-core-nlp">Ruby Gems page</a>). </li>
        <li>The larger <a href="https://github.com/louismullie/treat">TREAT</a> NLP toolkit by Louis Mullie also makes available Stanford CoreNLP. </li>
       </ul> 
       <h4>Perl</h4> 
       <ul> 
        <li><a href="https://metacpan.org/module/Lingua::StanfordCoreNLP">Perl wrapper</a> by Kalle Räisänen. </li>
       </ul> 
       <h4>Scala</h4> 
       <ul> 
        <li><a href="https://github.com/sistanlp/processors">Scala API for CoreNLP</a> by Mihai Surdeanu, one of the original developers of the CoreNLP package. </li>
       </ul> 
       <h4>Clojure</h4> 
       <ul> 
        <li><a href="https://github.com/gilesc/stanford-corenlp">Clojure wrapper for CoreNLP</a> by Cory Giles. Incomplete. Currently only a parser wrapper. </li>
        <li><a href="https://github.com/ngrunwald/stanford-nlp-tools">Clojure wrapper for CoreNLP</a> by Nils Grünwald. Incomplete. Currently only wraps tagger and TokensRegex. </li>
       </ul> 
       <h4><a name="JavaScript">JavaScript (node.js)</a></h4> 
       <ul> 
        <li><a href="https://npmjs.org/package/stanford-simple-nlp">stanford-simple-nlp</a> is a node.js CoreNLP wrapper by xissy (<a href="https://github.com/xissy/node-stanford-simple-nlp">github site</a>) </li> 
        <li><a href="https://www.npmjs.org/package/stanford-corenlp">stanford-corenlp</a>, a simply node.js wrapper by hiteshjoshi (<a href="https://github.com/hiteshjoshi/node-stanford-corenlp">github site</a>) </li> 
        <li><a href="https://github.com/mhewett/stanford-corenlp-node">stanford-corenlp-node</a> is a webservice interface to CoreNLP in node.js by Mike Hewett (<a href="https://github.com/mhewett/stanford-corenlp-node">github site</a>) </li> 
       </ul> 
       <h4>ZeroMQ server</h4> 
       <ul> 
        <li><a href="https://github.com/kowey/corenlp-server">corenlp-server</a>. Simple Java server communicating with clients via XML through ZeroMQ. Example Python client included. By Eric Kow. </li> 
       </ul> 
       <!-- https://github.com/westei/stanbol-stanfordnlp Standalone server with RESTful API that serves Apache Stanbol. --> 
       <!-- Support --> 
       <!--#set var="SOFTWARE_NAME" value="Stanford CoreNLP"--> 
       <h3><a name="Demo">Online Demo</a></h3> 
       <br> We have an 
       <a href="http://nlp.stanford.edu:8080/corenlp/">online demo</a> of CoreNLP which uses the default annotators. You can either see the xml output or see a nicely formatted xsl version of the output. 
       <br> 
       <h3><a name="History">Release History</a></h3> 
       <br> 
       <table> 
        <!--
<tr>
  <td><a href="stanford-corenlp-full-2015-04-20.zip">Version&nbsp;3.5.2</a></td>
  <td>2015-04-20</td>
  <td><a href="stanford-dependencies.html#universal">Universal Dependencies</a> as default representation; add Chinese coreference system</a></td>
    <td><a href="stanford-corenlp-caseless-2014-02-25-models.jar">caseless&nbsp;models</a></td>
    <td><a href="stanford-chinese-corenlp-2015-01-30-models.jar">chinese&nbsp;models</a></td>
    <td><a href="stanford-srparser-2014-10-23-models.jar">shift&nbsp;reduce&nbsp;parser&nbsp;models</a></td>
    <td><a href="stanford-spanish-corenlp-2015-01-08-models.jar">spanish&nbsp;models</a></td></tr>
--> 
        <tbody>
         <tr> 
          <td><a href="stanford-corenlp-full-2015-04-20.zip">Version&nbsp;3.5.2</a></td> 
          <td>2015-04-20</td> 
          <td>Switch to <a href="stanford-dependencies.shtml#universal">Universal Dependencies</a>, add Chinese coreference system to CoreNLP</td> 
          <td><a href="stanford-corenlp-caseless-2015-04-20-models.jar">caseless&nbsp;models</a></td> 
          <td><a href="stanford-chinese-corenlp-2015-04-20-models.jar">chinese&nbsp;models</a></td> 
          <td><a href="stanford-srparser-2014-10-23-models.jar">shift&nbsp;reduce&nbsp;parser&nbsp;models</a></td> 
          <td><a href="stanford-spanish-corenlp-2015-01-08-models.jar">spanish&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-full-2015-01-29.zip">Version&nbsp;3.5.1</a></td> 
          <td>2015-01-29</td> 
          <td>Substantial NER and dependency parsing improvements; new annotators for natural logic, quotes, and entity mentions</td> 
          <td><a href="stanford-corenlp-caseless-2014-02-25-models.jar">caseless&nbsp;models</a></td> 
          <td><a href="stanford-chinese-corenlp-2015-01-30-models.jar">chinese&nbsp;models</a></td> 
          <td><a href="stanford-srparser-2014-10-23-models.jar">shift&nbsp;reduce&nbsp;parser&nbsp;models</a></td> 
          <td><a href="stanford-spanish-corenlp-2015-01-08-models.jar">spanish&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-full-2014-10-31.zip">Version&nbsp;3.5.0</a></td> 
          <td>2014-10-31</td> 
          <td>Upgrade to Java 1.8; add annotators for <a href="nndep.shtml">dependency parsing</a>, relation extraction</td> 
          <td><a href="stanford-corenlp-caseless-2014-02-25-models.jar">caseless&nbsp;models</a></td> 
          <td><a href="stanford-chinese-corenlp-2014-10-23-models.jar">chinese&nbsp;models</a></td> 
          <td><a href="stanford-srparser-2014-10-23-models.jar">shift&nbsp;reduce&nbsp;parser&nbsp;models</a></td> 
          <td><a href="stanford-spanish-corenlp-2014-10-23-models.jar">spanish&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-full-2014-08-27.zip">Version&nbsp;3.4.1</a></td> 
          <td>2014-08-27</td> 
          <td>Spanish models added. <i>Last version to support Java 6 and Java 7.</i></td> 
          <td><a href="stanford-corenlp-caseless-2014-02-25-models.jar">caseless&nbsp;models</a></td> 
          <td><a href="stanford-chinese-corenlp-2014-02-24-models.jar">chinese&nbsp;models</a></td> 
          <td><a href="stanford-srparser-2014-08-28-models.jar">shift&nbsp;reduce&nbsp;parser&nbsp;models</a></td> 
          <td><a href="stanford-spanish-corenlp-2014-08-26-models.jar">spanish&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-full-2014-06-16.zip">Version&nbsp;3.4</a></td> 
          <td>2014-06-16</td> 
          <td>Shift-reduce parser and bootstrapped pattern-based entity extraction added</td> 
          <td><a href="stanford-corenlp-caseless-2014-02-25-models.jar">caseless&nbsp;models</a></td> 
          <td><a href="stanford-chinese-corenlp-2014-02-24-models.jar">chinese&nbsp;models</a></td> 
          <td><a href="stanford-srparser-2014-07-01-models.jar">shift&nbsp;reduce&nbsp;parser&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-full-2014-01-04.zip">Version&nbsp;3.3.1</a></td> 
          <td>2014-01-04</td> 
          <td>Bugfix release</td> 
          <td><a href="stanford-corenlp-caseless-2013-11-12-models.jar">caseless&nbsp;models</a></td> 
          <td><a href="stanford-chinese-corenlp-2013-11-12-models.jar">chinese&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-full-2013-11-12.zip">Version&nbsp;3.3.0</a></td> 
          <td>2013?11-12</td> 
          <td>Sentiment model added, minor sutime improvements, English and Chinese dependency improvements</td> 
          <td><a href="stanford-corenlp-caseless-2013-11-12-models.jar">caseless&nbsp;models</a></td> 
          <td><a href="stanford-chinese-corenlp-2013-11-12-models.jar">chinese&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-full-2013-06-20.zip">Version&nbsp;3.2.0</a></td> 
          <td>2013?06?20</td> 
          <td>Improved tagger speed, new and more accurate parser model</td> 
          <td><a href="stanford-corenlp-caseless-2013-06-07-models.jar">caseless&nbsp;models</a></td> 
          <td><a href="stanford-chinese-corenlp-2013-06-07-models.jar">chinese&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-full-2013-04-04.zip">Version&nbsp;1.3.5</a></td> 
          <td>2013?04?04</td> 
          <td>Bugs fixed, speed improvements, coref improvements, Chinese support </td>
          <td><a href="stanford-corenlp-caseless-2013-03-18-models.jar">caseless&nbsp;models</a></td> 
          <td><a href="stanford-chinese-corenlp-2013-04-08-models.jar">chinese&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-full-2012-11-12.zip">Version&nbsp;1.3.4</a></td> 
          <td>2012?11?12</td> 
          <td>Upgrades to sutime, dependency extraction code and English 3-class NER model</td> 
          <td><a href="stanford-corenlp-caseless-2012-11-09-models.jar">caseless&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-2012-07-09.tgz">Version&nbsp;1.3.3</a></td> 
          <td>2012?07?09</td> 
          <td>Minor bug fixes</td> 
          <td><a href="stanford-corenlp-caseless-2012-07-04-models.jar">caseless&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-2012-05-22.tgz">Version&nbsp;1.3.2</a></td> 
          <td>2012?05?22</td> 
          <td>Upgrades to sutime, include tokenregex annotator</td> 
          <td><a href="stanford-corenlp-caseless-2012-05-22-models.jar">caseless&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-2012-04-09.tgz">Version&nbsp;1.3.1</a></td> 
          <td>2012?04?09</td> 
          <td>Fixed thread safety bugs, caseless models available</td> 
          <td><a href="stanford-corenlp-caseless-2012-04-09-models.jar">caseless&nbsp;models</a></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-2012-01-08.tgz">Version&nbsp;1.3.0</a></td> 
          <td>2012?01?08</td> 
          <td>Fix a crashing bug, fix excessive warnings, threadsafe. <i>Last version to support Java 5.</i></td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-2011-09-14.tgz">Version&nbsp;1.2.0</a></td> 
          <td>2011?09?14</td> 
          <td>Added SUTime time phrase recognizer to NER, bug fixes, reduced library dependencies</td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-v1.1.0.tgz">Version&nbsp;1.1.0</a></td> 
          <td>2011?06?19</td> 
          <td>Greatly improved coref results</td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-v1.0.4.tgz">Version&nbsp;1.0.4</a></td> 
          <td>2011?05?15</td> 
          <td>DCoref uses less memory, already tokenized input possible</td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-v1.0.3.tgz">Version&nbsp;1.0.3</a></td> 
          <td>2011?04?17</td> 
          <td>Add the ability to specify an arbitrary annotator</td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-v1.0.2.tgz">Version&nbsp;1.0.2</a></td> 
          <td>2010?11?11</td> 
          <td>Remove wn.jar for license reasons</td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-v1.0.1.tgz">Version&nbsp;1.0.1</a></td> 
          <td>2010?11?10</td> 
          <td>Add the ability to remove XML</td>
         </tr> 
         <tr> 
          <td><a href="stanford-corenlp-v1.0.tar.gz">Version&nbsp;1.0</a></td> 
          <td>2010?11?01</td> 
          <td>Initial release</td>
         </tr> 
        </tbody>
       </table> 
      </div>
     </div>
    </div> 
   </div> 
  </div> 
  <footer class="widewrapper footer"> 
   <div class="container"> 
    <div class="row footer"> 
     <div class="col-sm-3"> 
      <h4>Stanford NLP Group</h4> Gates Computer Science Building
      <br> 353 Serra Mall
      <br> Stanford, CA 94305-9020
      <br> 
      <a href="http://forum.stanford.edu/visitors/directions/gates.php">Directions and Parking</a> 
     </div> 
     <div class="col-sm-3"> 
      <div class="indent30"> 
       <h4>Affiliated Groups</h4> 
       <ul class="grove-list"> 
        <li><a href="http://ai.stanford.edu/">Stanford AI Lab</a></li> 
        <li><a href="http://infolab.stanford.edu/">Stanford InfoLab</a></li> 
        <li><a href="https://www-csli.stanford.edu/">CSLI</a></li> 
       </ul> 
      </div> 
     </div> 
     <!--
	 <div class="col-sm-3">
	 </div> --> 
     <div class="col-sm-3"> 
      <div class="indent30"> 
       <h4>Connect</h4> 
       <!-- <div class="row"> --> 
       <ul class="grove-list"> 
        <li><a href="http://stackoverflow.com/tags/stanford-nlp">Stack Overflow</a></li> 
        <li><a href="https://github.com/stanfordnlp/CoreNLP">Github</a></li> 
        <li><a href="https://twitter.com/stanfordnlp">Twitter</a></li> 
       </ul> 
      </div> 
     </div> 
     <!--
	 <div class="row center-block">
           <a href="https://github.com/stanfordnlp/CoreNLP" class="social github"></a>
           <a href="https://twitter.com/stanfordnlp" class="social twitter"></a>
	 </div>
	 --> 
     <div class="col-sm-3"> 
      <div class="indent30"> 
       <h4>Local links</h4> 
       <div class="small" style="text-align:left"> 
        <a href="/local/nlp_lunch.shtml">NLP lunch</a> · 
        <a href="http://nlp.stanford.edu/read/">NLP Reading Group</a> 
        <br> 
        <a href="http://nlp.stanford.edu/seminar/">NLP Seminar</a> · 
        <a href="/local/calendar.shtml">Calendar</a> 
        <br> 
        <a href="/javanlp/">JavaNLP</a> (
        <a href="/nlp/javadoc/javanlp/">javadocs</a>) · 
        <a href="/local/machines.shtml">machines</a> 
        <br> 
        <a href="http://ai.stanford.edu/portfolio-view/distinguished-speaker-series">AI Speakers</a> · 
        <a href="/local/qa/">Q&amp;A</a> 
       </div> 
      </div> 
     </div> 
    </div> 
   </div> 
  </footer> 
  <script src="/static/js/vendor/bootstrap/bootstrap.min.js"></script> 
  <script src="/static/js/vendor/modernizr/modernizr.js"></script> 
  <script>
    $('ul.nav > li > a[href="' + document.location.pathname + '"]').parent().addClass('active');
</script> 
  <script>


  $('document').ready(function(){

  });

</script>  
 </body>
</html>