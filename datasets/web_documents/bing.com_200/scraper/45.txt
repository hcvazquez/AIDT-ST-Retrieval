<!--?xml version="1.0" encoding="UTF-8"?--><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
 <head> 
  <meta content="text/html; charset=utf-8" http-equiv="content-type"> 
  <title>bookied-scrapers 0.0.1 : Python Package Index</title> 
  <meta content="peerplays"> 
  <meta content="A library that allows to scrape APIs for bookie-related data"> 
  <link rel="alternate" type="application/rss+xml" title="RSS: 40 latest updates" href="https://pypi.python.org/pypi?:action=rss"> 
  <link rel="alternate" type="application/rss+xml" title="RSS: 40 newest packages" href="https://pypi.python.org/pypi?:action=packages_rss"> 
  <link rel="stylesheet" media="screen" href="/static/styles/screen-switcher-default.css" type="text/css"> 
  <link media="screen" href="/static/styles/netscape4.css" type="text/css" rel="stylesheet"> 
  <link media="print" href="/static/styles/print.css" type="text/css" rel="stylesheet"> 
  <link media="screen" href="/static/styles/largestyles.css" type="text/css" rel="alternate stylesheet" title="large text"> 
  <link media="screen" href="/static/styles/defaultfonts.css" type="text/css" rel="alternate stylesheet" title="default fonts"> 
  <link rel="stylesheet" media="screen" href="/static/css/docutils.css" type="text/css"> 
  <link rel="stylesheet" media="screen" href="/static/css/pygments.css" type="text/css"> 
  <!-- allow pypi to override the standard pydotorg/docutils/etc. styles --> 
  <link rel="stylesheet" href="/static/css/pypi.css" type="text/css"> 
  <link media="screen" rel="stylesheet" href="/static/css/pypi-screen.css" type="text/css"> 
  <meta name="google-site-verification" content="NSgF04qslVV4P7nymxJDSkWVK09zfdPTxgZfU3dNSoQ"> 
  <meta name="keywords" content="peerplays"> 
  <meta name="description" content="A library that allows to scrape APIs for bookie-related data"> 
  <link rel="meta" title="DOAP" type="application/rdf+xml" href="/pypi?:action=doap&amp;name=bookied-scrapers&amp;version=0.0.1"> 
  <style type="text/css">
  table.form th {white-space: pre;}
 </style> 
  <style type="text/css">
       </style> 
 </head> 
 <body> 
  <!--  Logo  --> 
  <h1 id="logoheader"> <a accesskey="1" href="http://www.python.org" id="logolink"> <img src="/static/images/python-logo.png" alt="homepage" border="0" id="logo"> </a> </h1> 
  <!--  Skip to Navigation  --> 
  <div class="skiptonav">
   <a accesskey="2" href="#left-hand-navigation"><img src="/static/images/trans.gif" alt="skip to navigation" border="0" id="skiptonav"></a>
  </div> 
  <div class="skiptonav">
   <a accesskey="3" href="#content-body"><img src="/static/images/trans.gif" alt="skip to content" border="0" id="skiptocontent"></a>
  </div> 
  <!--  Utility Menu  --> 
  <div id="utility-menu"> 
   <!--  Search Box  --> 
   <div id="searchbox"> 
    <form id="searchform" method="get" name="searchform" action="/pypi"> 
     <input type="hidden" name=":action" value="search"> 
     <div id="search"> 
      <input class="input-text" id="term" name="term" autofocus> 
      <input class="input-button" type="submit" name="submit" value="search" id="submit"> 
     </div> 
    </form> 
   </div> 
   <!-- XXX: reinstate this       <div id="screen-switcher"></div> --> 
  </div> 
  <div id="left-hand-navigation"> 
   <!--  Main Menu NEED LEVEL TWO HEADER AND FOOTER --> 
   <div id="menu"> 
    <ul class="level-one"> 
     <li class="selected"> <a class="selected" href="/pypi">Package Index</a> 
      <ul class="level-two"> 
       <li class=""><a class="" href="/pypi?%3Aaction=browse">Browse&nbsp;packages</a></li> 
       <li class=""><a class="" href="/pypi?%3Aaction=list_classifiers">List&nbsp;trove&nbsp;classifiers</a></li> 
       <li class=""><a class="" href="/pypi?%3Aaction=rss">RSS&nbsp;(latest&nbsp;40&nbsp;updates)</a></li> 
       <li class=""><a class="" href="/pypi?%3Aaction=packages_rss">RSS&nbsp;(newest&nbsp;40&nbsp;packages)</a></li> 
       <li><a href="/tos">Terms of Service</a></li> 
       <li><a href="http://wiki.python.org/moin/CheeseShopTutorial">PyPI Tutorial</a></li> 
       <li><a href="/security">PyPI Security</a></li> 
       <li><a href="http://sourceforge.net/tracker/?group_id=66150&amp;atid=513504">PyPI Support</a></li> 
       <li><a href="https://github.com/pypa/pypi-legacy/issues">PyPI Bug Reports</a></li> 
       <li><a href="http://www.python.org/sigs/distutils-sig/">PyPI Discussion</a></li> 
       <li><a href="http://wiki.python.org/moin/CheeseShopDev">PyPI Developer Info</a></li> 
      </ul> </li> 
     <li class=""><a href="http://www.python.org/about" class="" title="About The Python Language">About</a> </li>
     <li class=""><a href="http://www.python.org/news" class="" title="">News</a> </li>
     <li class=""><a href="http://www.python.org/doc" class="" title="">Documentation</a> </li>
     <li class=""><a href="http://www.python.org/download" title="">Download</a> </li>
     <li class=""><a href="http://www.python.org/community" class="" title="">Community</a> </li>
     <li class=""><a href="http://www.python.org/psf" class="" title="Python Software Foundation">Foundation</a> </li>
     <li class=""><a href="http://www.python.org/dev" class="" title="Python Core Language Development">Core Development</a> </li> 
    </ul> 
   </div> 
  </div> 
  <div id="content-body"> 
   <div id="body-main"> 
    <div id="content"> 
     <div id="breadcrumb"> 
      <a href="/pypi">Package Index</a> 
      <span class="breadcrumb-separator">&gt;</span> 
      <a href="/pypi/bookied-scrapers">bookied-scrapers</a> 
      <span class="breadcrumb-separator">&gt;</span> 
      <a href="/pypi/bookied-scrapers/0.0.1">0.0.1</a> 
     </div> 
     <div id="document-floating"> 
      <div id="document-navigation" style="overflow-y: auto; max-height: 15em; overflow-x: hidden;"> 
       <h4>Not Logged In</h4> 
       <ul> 
        <li><a href="/pypi?%3Aaction=login_form">Login</a></li> 
        <li><a href="/pypi?%3Aaction=register_form">Register</a></li> 
        <li><a href="/pypi?%3Aaction=forgotten_password_form">Lost Login?</a></li> 
        <li><a href="/openid_login">Login with OpenID</a> <a style="border: none;" href="/openid_login?provider=Launchpad"><img width="16" height="16" alt="Launchpad" src="https://launchpad.net/@@/launchpad.png" title="Launchpad"></a> </li> 
        <li><a href="/google_login">Login with Google<img width="16" height="16" src="https://www.google.com/favicon.ico" title="Google Login" alt="Google Login"></a></li> 
       </ul> 
       <div id="statusdiv"> 
       </div> 
      </div> 
     </div> 
     <div class="section"> 
      <h1>bookied-scrapers 0.0.1</h1> 
      <div id="download-button"> 
       <a class="button green" style="float:right;" href="#downloads">Downloads ?</a> 
      </div> 
      <p style="font-style: italic">A library that allows to scrape APIs for bookie-related data</p> This file contains instructions on how to install and use the feed_fetcher as well as describing its structure
      <br>
      <br>USAGE
      <br>=====
      <br>
      <br>Install python, pip, virtualenv and virtualenvwrapper (see example below for
      <br>Ubuntu)
      <br>
      <br>Run:
      <br> $ apt-get install python3-dev python-pip \
      <br> virtualenv virtualenvwrapper \
      <br> libxml2-dev libxslt1-dev python-mysqldb \
      <br> mysql-client mysql-server \
      <br> aufs-tools
      <br>
      <br>Then run the following to complete the set-up:
      <br>
      <br> $ mkvirtualenv feed_fetcher -p $(which python3)
      <br> $ workon feed_fetcher
      <br> $ pip install -r environments/base.txt
      <br>
      <br>The feed_fetcher will output to stdout and also into a database. To set up the
      <br>database do the following (I configured db_peerplays with user peerplays and
      <br>password "I&lt;3storage"):
      <br> - access mysql as root and perform the following (mysql -u root -p):
      <br> &gt; CREATE USER 'peerplays'@'localhost' IDENTIFIED BY 'I&lt;3storage';
      <br> &gt; CREATE DATABASE db_peerplays;
      <br> &gt; GRANT ALL PRIVILEGES ON db_peerplays.* TO 'peerplays'@'localhost';
      <br> &gt; FLUSH PRIVILEGES;
      <br>
      <br>Install docker:
      <br>Instructions for Ubuntu: https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/#recommended-extra-packages-for-trusty-1404
      <br>Or better (from https://www.liquidweb.com/kb/how-to-install-docker-on-ubuntu-14-04-lts/):
      <br> $ sudo apt-get install docker.io
      <br>
      <br>NOTE: Docker will not work correctly on a 32bit machine. Make sure you run this on 64bit Linux.
      <br>
      <br>Run docker:
      <br> $ sudo /etc/init.d/docker start # If not configured to start automatically
      <br> $ sudo docker run -p 8050:8050 scrapinghub/splash --max-timeout 3600
      <br>
      <br>To change configuration, including database details, edit data_monitor/config.py
      <br>
      <br>Then every time you want to use the feed_fetcher just re-run the workon command
      <br>followed by:
      <br> $ python ./sample_apps/live_app.py
      <br>
      <br>After running some tests you can inspect the SQLite database by running some commands on linux, like the following:
      <br> $ sqlite3 db_peerplays '.tables'
      <br> $ sqlite3 db_peerplays 'SELECT * FROM resolvedgameevent'
      <br> $ sqlite3 db_peerplays 'SELECT * FROM rawgameinfo'
      <br> $ sqlite3 db_peerplays 'SELECT * FROM rawevent'
      <br>
      <br>STRUCTURE
      <br>=========
      <br>
      <br>The data scraping mechanism is revolves around the Engine. It's what the
      <br>application communicates with and what manages all the plug-ins. 
      <br>
      <br>Normally the application will create an Engine instance, will provide some
      <br>Scraper plug-ins, some Processing plug-ins and some Data Sink plug-ins. Also
      <br>the Engine will be configured to run as desired. Once the set-up is done, the
      <br>app will just start the engine which will run until called to stop (this
      <br>usually happens on interrupt).
      <br>
      <br>The way the Engine runs is the following:
      <br> - asks all Data Sinks if there are stored games. Usually a database Sink
      <br> would have some games stored from a previous run
      <br> - sets up the Scrapers with the callbacks and all the intervals that are
      <br> configured
      <br> - at the right intervals asks the Scrapers to check for scheduled games, or
      <br> live games
      <br> - when the Scrapers provide some game details, they are passed through all
      <br> the Processors
      <br> - each Processor may correct things, like team names, or even reject a
      <br> game/event
      <br> - once processed, the Engine will check if this is a new event (like score
      <br> changed, game status changed etc). If not new, does nothing more.
      <br> - the Engine passes the event to all the Data Sinks
      <br> - each Data Sink may choose to trigger a conflict resolution request.
      <br> Typically database Data Sinks will be doing this, as they can check stored
      <br> info from all Scrapers.
      <br> - the Engine resolves any conflicts and then send the resolution to all the
      <br> Data Sinks
      <br>
      <br>To aid all this, data defined in the data module is used to store the
      <br>information.
      <br>
      <br>CONTENTS
      <br>========
      <br>
      <br>- data_monitor - the main component of the data monitor. Contains the API to
      <br> the engine, to the plug-ins and the data models
      <br>- sample_apps - example of applications using the data_monitor API. There are
      <br> a couple of test apps which do not download data, and a live
      <br> application making use of all scrapers
      <br>- data_sinks - data sink plug-ins. contain a sample console one and a database
      <br> plug-in
      <br>- processors - example processors. Has a processor which makes use of the info
      <br> stored in the yaml files of the witness lookup script
      <br>- scrapers - example scraper plug-ins. Apart from the test one there are some
      <br> scrapers which can be used to get data from some websites
      <br>
      <br>Others:
      <br>- environments - the pip environments which need to be installed. The base.txt
      <br> is enough
      <br>- lab - some old experimentation stuff
      <br>- AUTHORS, COPYING, README - informational files, copyright info etc.
      <br>
      <br>DETAILS
      <br>=======
      <br>
      <br>* The handicaps list can have 2 or 3 items. The third one is needed when
      <br> a draw handicap is presented. If that value is the same as the away
      <br> handicap, this can look as duplicated.
      <br>
      <br>* rawmarketsinfo has a foreign key to rawgameinfo which contains the
      <br> scraper information. Each raw market event must have a raw game
      <br> associated.
      <br>
      <br>* An event has a created_at timestamp. Each time something changes (game
      <br> status, or score) a new event row is added. So all the event
      <br> timestamps are preserved. A game is created when the first event is
      <br> created (games are always reported with an event containing status and
      <br> score). For the market we don't store timestamps, but they would have
      <br> the timestamp of the latest event because when markets are reported the
      <br> event status is also given. 
      <br>
      <br>* The conflict_resolver.py file which is in the same module as the
      <br> engine implementation deals with sources disagreement. The raw tables
      <br> have the detailed information for each scraper. When some update happens
      <br> this information is passed through the resolver. The resolver will
      <br> output what it considers to be most accurate state together with how
      <br> many sources agree on that result. This is stored in resolvedgameevent
      <br> and it has some special columns, like reported_count (how many sources
      <br> reported the resolved score &amp; status) and total_count (how many sources
      <br> reported on the game). If you would want to see what each scraper
      <br> reported, you would need a query to the rawgameinfo and rawevent.
      <br>
      <br>* Please note that while rawevent stores all the events, the
      <br> resolvedgameevent only stores the latest state as from what we
      <br> understood this is what was necessary. 
      <br>
      <br>PROJECT PLAN
      <br>============
      <br>1. Project Plan &amp; Milestones
      <br>
      <br>## New scope
      <br>As of 11th of July 2017 the scope of the project has changed. For now we will deliver a
      <br>Python module which will retrieve data from feeds, normalize, and store them in a database.
      <br>This will be used as part of the larger Witness Scripts middle layer.
      <br>
      <br>## Milestones
      <br>The milestones for the development of these module are:
      <br>1. 1st of August: Functional prototype
      <br>2. 1st of September: Release version of the module
      <br>
      <br>## Requirements
      <br>For the prototype:
      <br>• The application shall run as a stand-alone application.
      <br>• The application can be configured to access a certain feed from a list of supported feeds.
      <br>• The application will initially only work with baseball result feeds
      <br>• The feeds will be monitored based on polling. If a feed offers some sort of push mechanism, or auto-refresh, this will not be used.
      <br>• The polling time will be configurable.
      <br>• Not all the feeds which will be needed for the end product will be covered by the application. Only a subset of the feeds will be available (e.g., one of each type).
      <br>• The application will be monitoring the feeds periodically and when a new result is reported, this result will be inserted in a database. Note that only final results are going to be handled, no in-game events, like points scored, sending off, etc.
      <br>• The match results will be normalized based on a set of rules which guarantee that the team names and match times are consistent with Peerplays’ specs. Specifically:
      <br> 1. Event de-duplication
      <br> 2. Date Transformation into a uniform representation that maps 1-to-1 onto the predefined object structure (defined in bitbucket wiki)
      <br> 3. Event start times given in UTC
      <br> 4. Unified spellings created according to Witness Lookup
      <br>• To aid normalization, user configuration is to be expected, for example, providing knowledge-base of team aliases.
      <br>• It will be straight-forward to insert items to the knowledgebase.
      <br>
      <br>For the release version of the module:
      <br>• The prototype will be extended to cover all the baseball feeds needed.
      <br>• The normalization algorithm will be improved to detect teams, and other elements, without needing an extensive knowledgebase. Ideally, this phase should require zero input from the user.
      <br>• The normalization phase will check for completeness of data
      <br>• The normalization algorithm will record aliases it deduced, while also allowing the user to manually intervene and correct mislabeled teams/results.
      <br>• A Python API for the module can be provided allowing integration of the module directly within a larger application.
      <br>• The API will allow the handling of multiple data sinks so normalized results can be directed as needed (not necessarily a database).
      <br>• In-game events will be supported as well as game results.
      <br>
      <br>## Architecture
      <br>An architecture has been defined and is available on request.
      <br>
      <br>2. Deliverables and Costs
      <br>New scope: feed fetcher deliverables
      <br>For the first phase of the project, i.e., the functional prototype, the following will be delivered:
      <br>• Binaries comprising an application which will be continuously running and retrieving information from a configured feed and storing normalized values within the designated database
      <br>• Documentation on how to use and configure the application
      <br>• Optional for prototype (due to time constraints): the module can be provided as a library with an API, rather than as a stand-alone application. This is probably not needed, but if there is a need to include the module in a larger application it can be achieved as part of the release version.
      <br>
      <br>The delivery of the above prototype components will be charged at €12,000. Once the
      <br>prototype is delivered we will discuss about the way we will proceed with the project.
      <br> 
      <a name="downloads">&nbsp;</a> 
      <table class="list" style="margin-bottom: 10px;"> 
       <tbody>
        <tr> 
         <th>File</th> 
         <th>Type</th> 
         <th>Py Version</th> 
         <th>Uploaded on</th> 
         <th style="text-align: right;">Size</th> 
        </tr> 
        <tr class="odd"> 
         <td> <span style="white-space: nowrap;"> <a href="https://pypi.python.org/packages/5b/c1/5c0834fda917099c19d028bda103ea054c6b1e4b3bb49fca9fd16927b3bd/bookied-scrapers-0.0.1.tar.gz#md5=77974240505798c25195073dbcbc3d3a">bookied-scrapers-0.0.1.tar.gz</a> (<a title="MD5 Digest" href="/pypi?:action=show_md5&amp;digest=77974240505798c25195073dbcbc3d3a">md5</a>) </span> </td> 
         <td style="white-space: nowrap;"> Source </td> 
         <td> </td> 
         <td>2017-10-06</td> 
         <td style="text-align: right;">33KB</td> 
        </tr> 
        <tr class="even"> 
         <td> <span style="white-space: nowrap;"> <a href="https://pypi.python.org/packages/e2/0a/93da4ebcf6b883ecfa99c81baa22f04dd871a0f7f550675e50f9bdc65e8b/bookied_scrapers-0.0.1-py3-none-any.whl#md5=18efa06d66f29e9f5dfbb0b38b6b7101">bookied_scrapers-0.0.1-py3-none-any.whl</a> (<a title="MD5 Digest" href="/pypi?:action=show_md5&amp;digest=18efa06d66f29e9f5dfbb0b38b6b7101">md5</a>) </span> </td> 
         <td style="white-space: nowrap;"> Python Wheel </td> 
         <td> 3.6 </td> 
         <td>2017-10-06</td> 
         <td style="text-align: right;">51KB</td> 
        </tr> 
        <tr>
         <td id="last" colspan="6"></td>
        </tr> 
       </tbody>
      </table> 
      <ul class="nodot"> 
       <li> <strong>Author:</strong> <span>Fabian Schuh</span> </li> 
       <!-- The <th> elements below are a terrible terrible hack for setuptools --> 
       <li> <strong>Home Page:</strong> 
        <!-- <th>Home Page --> <a href="http://pbsa.info">http://pbsa.info</a> </li> 
       <li> <strong>Download URL:</strong> 
        <!-- <th>Download URL --> <a href="https://github.com/pbsa/bookie-scrapers/tarball/0.0.1">https://github.com/pbsa/bookie-scrapers/tarball/0.0.1</a> </li> 
       <li> <strong>Keywords:</strong> <span>peerplays</span> </li> 
       <!-- TODO: add link to products in follow dependencies... --> 
       <li> <strong>Categories</strong> 
        <ul class="nodot"> 
         <li> <a href="/pypi?:action=browse&amp;c=3">Development Status :: 3 - Alpha</a> </li> 
         <li> <a href="/pypi?:action=browse&amp;c=30">Intended Audience :: Developers</a> </li> 
         <li> <a href="/pypi?:action=browse&amp;c=69">License :: OSI Approved :: MIT License</a> </li> 
         <li> <a href="/pypi?:action=browse&amp;c=156">Operating System :: OS Independent</a> </li> 
         <li> <a href="/pypi?:action=browse&amp;c=533">Programming Language :: Python :: 3</a> </li> 
        </ul> </li> 
       <li> <strong>Package Index Owner:</strong> <span>xeroc</span> </li> 
       <li> <strong><a href="https://github.com/edumbill/doap/wiki">DOAP</a> record:</strong> <a href="/pypi?:action=doap&amp;name=bookied-scrapers&amp;version=0.0.1">bookied-scrapers-0.0.1.xml</a> </li> 
      </ul> 
     </div> 
    </div> 
    <div id="footer"> 
     <div id="credits"> 
      <div style="float: left; margin-right: 1em;" id="badges"> 
       <img src="https://img.shields.io/badge/ipv6-go!-green.svg" alt="ipv6 ready" title="ipv6 ready" border="0">
       <br> 
       <img src="https://img.shields.io/badge/http2-go!-green.svg" alt="http2 ready" title="http2 ready" border="0">
       <br> 
       <img src="/static/images/PythonPoweredAnimSmall.gif" alt="darn right it is" title="Python Powered" border="0"> 
      </div> 
      <div style="float: right" id="donations"> 
       <a href="http://www.python.org/about/website">Website maintained by the Python community</a>
       <br> 
       <a href="https://www.fastly.com/" title="Real-time CDN services provided by Fastly">Real-time CDN by Fastly</a> / 
       <a href="http://developer.rackspace.com/" title="Server hosting by Rackspace Open Source support">Hosting by Rackspace</a>
       <br> 
       <a href="https://aws.amazon.com/s3/" title="Object storage provided by Amazon S3">Object storage by Amazon S3</a> / 
       <a href="http://www.timparkin.co.uk/" title="Design by Tim Parkin, Yorkshire man, photographer and developer">Design by Tim Parkin</a> 
      </div> 
     </div> Copyright © 1990-2017, 
     <a href="http://www.python.org/psf">Python Software Foundation</a>
     <br> 
     <a href="https://pypi.org/policy/terms-of-use/">Terms of Use</a> 
    </div> 
   </div> 
  </div> 
  <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-55961911-1', 'auto');
        ga('require', 'linkid', 'linkid.js');
        ga('send', 'pageview');
      </script> 
  <script type="text/javascript" src="//statuspage-production.s3.amazonaws.com/se-v2.js">
      </script> 
  <script type="text/javascript">
        var sp = new StatusPage.page({ page : '2p66nmmycsj3' });
        sp.summary({
          // <![CDATA[
          success: function(data) {
            var div = document.getElementById('statusdiv');
            var reports = "</br><h4 id='statusbox'>Status</h4>\n";
            var outage = 0;
            var maintenance = 0;
            for (i in data.incidents) {
              var incident = data.incidents[i];
              var message, status = incident.status;
              if (status === 'scheduled') {
                message = '<li><a href="' + incident.shortlink + '">' + incident.name + ' scheduled.</a></li>\n';
                reports += message;
                maintenance += 1;
              } else if (status === 'in_progress') {
                message = '<li><a href="' + incident.shortlink + '">' + incident.name + ' is currently in progress.' + '</a></li>\n';
                reports += message;
                maintenance += 1;
              } else if (status !== 'resolved' && status !== 'postmortem' && status !== 'completed') {
                message = '<li><a href="' + incident.shortlink + '">' + incident.name + ': ' + incident.status + '</a></li>\n';
                reports += message;
                outage += 1;
              }
            }
            for (i in data.scheduled_maintenances) {
              var incident = data.scheduled_maintenances[i];
              var message, status = incident.status;
              if (status === 'scheduled') {
                message = '<li><a href="' + incident.shortlink + '">' + incident.name + ' scheduled.</a></li>\n';
                reports += message;
                maintenance += 1;
              } else if (status === 'in_progress') {
                message = '<li><a href="' + incident.shortlink + '">' + incident.name + ' is currently in progress.' + '</a></li>\n';
                reports += message;
                maintenance += 1;
              } else if (status !== 'resolved' && status !== 'postmortem' && status !== 'completed') {
                message = '<li><a href="' + incident.shortlink + '">' + incident.name + ': ' + incident.status + '</a></li>\n';
                reports += message;
                outage += 1;
              }
            }
            if (outage + maintenance === 0) {
              reports += "<li><a href='http://status.python.org'>Nothing to report</a></li>";
            }
            div.innerHTML=reports;
            if (outage > 0) {
              var statusbox = document.getElementById("statusbox");
              statusbox.style.background = '#FC234A';
            }
          }
          // ]]>
        });
      </script>   
 </body>
</html>